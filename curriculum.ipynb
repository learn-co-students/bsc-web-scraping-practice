{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6c8c3c7",
   "metadata": {},
   "source": [
    "## Scraping info about the BSC Faculty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886f7bce-3bd7-4ebd-be5b-28425fdee68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell unchanged\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9fea0d-4894-4323-9821-3b935e45e7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#__SOLUTION__\n",
    "# Run this cell unchanged\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc95459f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell unchanged\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from time import sleep\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import chromedriver_autoinstaller\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cf2a70-ced4-4618-b75b-7a3ff447cbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#__SOLUTION__\n",
    "# Run this cell unchanged\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from time import sleep\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import chromedriver_autoinstaller\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9665a73-f5a9-4471-aa99-3089a536933d",
   "metadata": {},
   "source": [
    "**The goal for this notebook is to scrape the..**\n",
    "* Name\n",
    "* Office \n",
    "* Office phone\n",
    "* Email\n",
    "* The classes taught by the teacher\n",
    "\n",
    "**...for each faculty member that works for the programs linked [here](https://www.bsc.edu/academics/index.html)**\n",
    "\n",
    "\n",
    "For this scraping problem, we will want to create two seperate url variables\n",
    "1. The root url\n",
    "2. The url for initial webpage.\n",
    "    * This is the root url + some forward slashed words that point to the page we are looking for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e82e599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the root url\n",
    "\n",
    "# Set up the url we want to make requests to.\n",
    "\n",
    "# Send request\n",
    "\n",
    "# Print the request response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0418c3-5dbf-4786-95ce-6a6b6841b701",
   "metadata": {},
   "outputs": [],
   "source": [
    "#__SOLUTION__\n",
    "# Set up the root url \n",
    "root = 'https://www.bsc.edu/academics/'\n",
    "# Set up the url we want to make requests to. \n",
    "url = root + 'index.html'\n",
    "# Send request\n",
    "response = get(url)\n",
    "# Print response\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4d65b5-9f64-4d36-8b2a-3b1c44c543ef",
   "metadata": {},
   "source": [
    "It looks there are some security measures that are keeping us from making a sucessful request.\n",
    "\n",
    "One common way around this is to use a python package called `selenium`.\n",
    "\n",
    "Below is some code to set up a selenium webdriver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c779e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_driver(headless=True):\n",
    "    driver = chromedriver_autoinstaller.install(cwd=True)\n",
    "    chrome_options = webdriver.ChromeOptions()     \n",
    "    if headless:\n",
    "        chrome_options.add_argument(\"--headless\")\n",
    "\n",
    "    driver = webdriver.Chrome(driver, \n",
    "                             chrome_options = chrome_options)\n",
    "    return driver\n",
    "\n",
    "driver = create_driver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63e7457-085d-4d10-94e7-366979ff1c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#__SOLUTION__\n",
    "def create_driver(headless=True):\n",
    "    driver = chromedriver_autoinstaller.install(cwd=True)\n",
    "    chrome_options = webdriver.ChromeOptions()     \n",
    "    if headless:\n",
    "        chrome_options.add_argument(\"--headless\")\n",
    "\n",
    "    driver = webdriver.Chrome(driver, \n",
    "                             chrome_options = chrome_options)\n",
    "    return driver\n",
    "\n",
    "driver = create_driver()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb14610f-aed2-45f0-b059-8b0e675e26b9",
   "metadata": {},
   "source": [
    "Now we can run the code below, and we'll be able to connect to the web page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d791395e",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = 'https://www.bsc.edu/academics/'\n",
    "url = root + 'index.html'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c703bbf-0c38-47e0-90f3-0096956e339b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#__SOLUTION__\n",
    "root = 'https://www.bsc.edu/academics/'\n",
    "url = root + 'index.html'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa75d42-0d9c-44f7-a2dd-71c3765d36fb",
   "metadata": {},
   "source": [
    "To pull the raw html from our webdriver, we can run the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ec5df0-864e-494e-8ce2-b994e167d927",
   "metadata": {},
   "outputs": [],
   "source": [
    "html = driver.page_source\n",
    "\n",
    "print('HTML:')\n",
    "print(html[:150])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059df064-68fa-4bfd-b6be-9cf5e3bd0d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#__SOLUTION__\n",
    "html = driver.page_source\n",
    "\n",
    "print('HTML:')\n",
    "print(html[:150])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae5c791-bca8-4163-ad8a-0ddc19d6a454",
   "metadata": {},
   "source": [
    "Next we pass the html into `BeautifulSoup` so we can parse the information embedded inside the html. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eecccaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass html into BeautifulSoup\n",
    "soup = BeautifulSoup(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721fe6af-06cd-4e2b-974a-3a0c5db53ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#__SOLUTION__\n",
    "# Pass html into BeautifulSoup\n",
    "soup = BeautifulSoup(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d985a8-f102-4171-828c-2ceffb9fcea5",
   "metadata": {},
   "source": [
    "**Ok now let's start scraping.**\n",
    "\n",
    "For this page, we need to collect the name of the department and the url for each department's landing page. \n",
    "\n",
    "Let's create a dictionary that has the following format:\n",
    "\n",
    "```\n",
    "{department_name: deparment_url}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab2642d-957b-4fc4-bf8c-4a1c76c7b6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3974543-41ec-474e-aa2c-109d2cefa9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#__SOLUTION__\n",
    "div = soup.find_all('h2', text=re.compile('Academic Departments'))[0].parent\n",
    "a_tags = div.find_all('a')\n",
    "\n",
    "department_links = {}\n",
    "for tag in a_tags:\n",
    "    department_links[tag.text] = root + tag.attrs['href']\n",
    "\n",
    "department_links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086cd809-f913-4411-a4ce-61f4cb30b854",
   "metadata": {},
   "source": [
    "**Next, we need to collect the html for each of the department landing pages**\n",
    "\n",
    "Let's create a dictionary with the following format:\n",
    "\n",
    "```\n",
    "{department_name: department_html}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93eb8f28-e73c-4e17-a17d-829f523e253d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9ab624",
   "metadata": {},
   "outputs": [],
   "source": [
    "#__SOLUTION__\n",
    "department_html = {}\n",
    "\n",
    "for department in department_links:\n",
    "    link = department_links[department]\n",
    "    driver.get(link)\n",
    "    department_html[department] = driver.page_source\n",
    "    sleep(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cadef6a-a159-4205-84e9-7d3f77c5e290",
   "metadata": {},
   "source": [
    "Now let's create a dictionary containing each departments `soup` (or their parsed html files)\n",
    "\n",
    "```\n",
    "{department_name: department_soup}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c1d52d-bfa0-495d-b1b3-d23f0d5420f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e277db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#__SOLUTION__\n",
    "department_soup = {}\n",
    "\n",
    "for department in department_html:\n",
    "    soup = BeautifulSoup(department_html[department])\n",
    "    department_soup[department] = soup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b83a89b-7f89-462e-a31b-2ed1beb68a06",
   "metadata": {},
   "source": [
    "Ok. So at this point, we have successfully collect the parsed html for each department's landing page. \n",
    "\n",
    "Now, we need to figure out how to write code that will scrape the information for each faculty member found on the department's landing page. The problem we will discover is that none of the landing pages are formatted exactly the same. *This* is where the true messying of web scraping appears. **The majority of the work is trial and error**, where you will write code that works for a specific page, apply the code to another page, and see if it fails. If it does, you then adjust your scraping code so it can handle *both* pages and whatever differences exist between them. \n",
    "\n",
    "**It is important** to recognize that when we are webscraping we are dealing with human desicision making. Somewhere out there in the world, there is a web designer who built this website, and they made decisions about how they'd like to organize the webpage. Sometimes, at least from the perspective of a web scraper, the web designer did a bad job and the organization of the page will seem totally non sensical. Sometimes, the web designer intentionally made the website organization inconsistant because they do not want people to scrape their webpage. When it comes to web scraping, much of the work is simply trying to make sense of someone's messy office. \n",
    "\n",
    "**In the cell below, let's isolate the soup for the `Sociology` department.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bbd9a5-db20-42e2-b0ad-605ddcdfd697",
   "metadata": {},
   "outputs": [],
   "source": [
    "program = department_soup['Sociology']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f33b78e-2822-4421-a510-5455da84211e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#__SOLUTION__\n",
    "program = department_soup['Sociology']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8162ff-9f2d-43c5-a09b-7eb163d28e94",
   "metadata": {},
   "source": [
    "Now we need to isolate the section of the html that contains the information we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2cda7d-34d5-4887-9283-2431202f3a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef6a59f-24c9-46a2-8ce0-9a5174bf7936",
   "metadata": {},
   "outputs": [],
   "source": [
    "#__SOLUTION__\n",
    "dropdown = program.find('section', {'class': 'accordion'})\n",
    "faculty_drop = dropdown.find('h2', text=re.compile('STAFF|FACULTY|Faculty|Staff')).find_previous('li')\n",
    "faculty = faculty_drop.find('div', {'class': 'content'}).find_all('p')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7844e7e3-62e7-41bc-9f2d-284511cc6520",
   "metadata": {},
   "source": [
    "Ok. This is where is gets really hairy. \n",
    "\n",
    "We need to:\n",
    "* Loop over the tags inside the `faculty` variable\n",
    "* Collect the name of the staff member\n",
    "* Check if the staff member has a link for a bio\n",
    "    * If they do collect the url for that page, collect html and parse it\n",
    "    * Scrape office, phone, email, and classes from the bio\n",
    "* If not, see if we can find any of the data points in the faculty member's section of the program landing page.\n",
    "* Append the data we have scraped for a staff member to a list\n",
    "\n",
    "Below I have writting some code to handle some of the tricky elements. **Please know** that these `\"tricky elements\"` were not immediately obvious when I was writing the code! It took lots of trial and error to figure out a working solution, and even still, the solution isn't totally perfect. The code for these `\"tricky elements\"` is provided for the sake of making this assignment time efficient, but this process of trial and error is a very essential part of web scraping. When you have the time, I encourage you to copy the code into a code cell, and try to figure out what it is doing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a00223-514c-46e9-9a6e-ce251009ab6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is completed for you\n",
    "def parse_faculty_no_link(p_tag):\n",
    "    \"\"\"\n",
    "    Helper function for collecting bio data \n",
    "    for staff members who have placed their bio\n",
    "    in the program faculty dropdown\n",
    "    \"\"\"\n",
    "    phone = p_tag.find(text = re.compile('\\(.+\\) \\d{3}-\\d{4}|\\d{3}-\\d{3}-\\d{4}'))\n",
    "    if phone:\n",
    "        phone = phone.strip()\n",
    "    email = p_tag.find(text = re.compile('\\@bsc\\.edu'))\n",
    "    return phone, email\n",
    "\n",
    " \n",
    "# Now let's write code collect the data for every faculty member\n",
    "# in a department!\n",
    "\n",
    "data = []\n",
    "root = 'https://www.bsc.edu/academics/'\n",
    "\n",
    "# Loop over the tags in faculty\n",
    "for person in faculty:\n",
    "\n",
    "    # Create a default dataset for a staff member\n",
    "    # that sets the default values to None\n",
    "    # That way, if we do not find anything, \n",
    "    # we have a null value for the observation\n",
    "    default = {\"name\": None,\n",
    "               \"office\": None,\n",
    "              \"phone\": None,\n",
    "              \"email\": None,\n",
    "              \"classes\": None}\n",
    "\n",
    "    # Collecting the faculty name turned out to be quite difficult\n",
    "    # The departments deviate from each other pretty dramatically\n",
    "    # in the way they format their faculty dropdown.\n",
    "    # the `.string` attribute was used because it returns\n",
    "    # the strings inside the person div as seperate objects\n",
    "    # Whereas `.text` returns one joined string. This poses a problem\n",
    "    # When faculty members included a bio in the same tag as their name.\n",
    "    name = list(person.strings)\n",
    "    if not name:\n",
    "        # Some p tags were empty. \n",
    "        # This if check ensures we ignore them.\n",
    "        continue\n",
    "    if len(name[0]) > 150:\n",
    "        # If the length of the text is >150, it is likely a bio of some kind\n",
    "        # So we skip this p tag and move to the next\n",
    "        continue\n",
    "    if not name[0].strip():\n",
    "        # If, when we strip the string (remove preceding and trailing spaces)\n",
    "        # The string becomes empty, it is just a ptag filled with white space\n",
    "        try:\n",
    "            # In some cases, the ptag opened with a white space\n",
    "            # with a second nested ptag where the instructors name \n",
    "            # was found. If the first tag was white space\n",
    "            # We are naively setting the name to the second string\n",
    "            # This has been placed inside a try except block in case\n",
    "            # a second string doesn't exist. In which case, \n",
    "            # the ptag is completely\n",
    "            # empty and we jump to the next iteration of ptags\n",
    "            name = name[1]\n",
    "        except:\n",
    "            continue\n",
    "    else:\n",
    "        # If the stripped string is not empty and is not >150\n",
    "        # Then we assume the string is the name of the staff member\n",
    "        name = name[0]\n",
    "    # We set the name data to the scraped string\n",
    "    default['name'] = name\n",
    "\n",
    "    # Next we check if the ptag contains any `a` tags (links)\n",
    "    if person.find('a'):\n",
    "        if person.find('a').attrs['href'][:2] == '..':\n",
    "        # If an `a` tag was found, we check if the href for the atag\n",
    "        # contains the string '..'\n",
    "        # This is a weird design choice by the web developers\n",
    "        # Where they place '..' at the beginning of relative urls.\n",
    "        # Ultimately, if the url is relative it is most likely the bio page\n",
    "        # for the staff member. \n",
    "            # If a relative path was found\n",
    "            # We collect the href and assemble the url\n",
    "            # for the staff member's bio page \n",
    "            href = person.find('a').attrs['href']\n",
    "            faculty_url = root + href.replace('../', '')\n",
    "            # Next we connect to the web page,\n",
    "            # collect the html, and parse it.\n",
    "            driver.get(faculty_url)\n",
    "            faculty_html = driver.page_source\n",
    "            faculty_soup = BeautifulSoup(faculty_html)\n",
    "            try:\n",
    "                pass\n",
    "                # The entire next bit is wrapped in a try except\n",
    "                # This is mostly to keep our code somewhat more readible\n",
    "                # If we find that we are losing too much data \n",
    "                    # For example if `office` is throwing a lot of errors\n",
    "                    # which means none of the other data points are collected\n",
    "                # We could consider wrapping each of these datapoints in their\n",
    "                # own try except block\n",
    "                \n",
    "                # ================= STUDENT WORK =======================\n",
    "                # Find the office for the staff member\n",
    "                # YOUR CODE HERE\n",
    "                \n",
    "                # Set the office variable in our default dictionary \n",
    "                # to the office data\n",
    "                # YOUR CODE HERE\n",
    "                \n",
    "                # Find the phone number for the staff member\n",
    "                # YOUR CODE HERE\n",
    "                \n",
    "                # Set the phone variable in our default dictionary \n",
    "                # to the phone data\n",
    "                # YOUR CODE HERE\n",
    "                \n",
    "                # Find the email for staff member\n",
    "                # YOUR CODE HERE\n",
    "                \n",
    "                # Set the email variable in our default dictionary \n",
    "                # to the email data\n",
    "                # YOUR CODE HERE\n",
    "                \n",
    "                # Find the classes taught by the staff member\n",
    "                # YOUR CODE HERE\n",
    "                \n",
    "                # Set the classes variable in our default dictionary \n",
    "                # to the classes data\n",
    "                # YOUR CODE HERE\n",
    "                \n",
    "            except:\n",
    "                # If an error is thrown move to the last line in the for loop\n",
    "                pass\n",
    "        else:\n",
    "            # If no a tag was found, we have a helper function\n",
    "            # parse the information of the staff member in the drop down\n",
    "                # Some programs have opted to place their entire bio\n",
    "                # in the dropdown and do not seem to have a seperate web page\n",
    "            phone, email = parse_faculty_no_link(person)\n",
    "            default['phone'] = phone\n",
    "            default['email'] = email\n",
    "    # Append the found data for a staff member to a list\n",
    "    data.append(default)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4dc79d-ad60-4ade-8365-41b3741df2ff",
   "metadata": {},
   "source": [
    "Let's take a look at the results..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5f42c4-0114-40d0-9cfa-a0fd35a6fcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a00eb6-1dee-4ab3-a3f4-3e391a677ef5",
   "metadata": {},
   "source": [
    "This looks good. Now let's put it all together!\n",
    "\n",
    "To make our web scraping code *reusable*, which I an extremely important part of writing powerful web scraping, we need to toss this entire process into functions. \n",
    "\n",
    "**Please create a functions below:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9c5055-e7b1-4df9-81b1-b46d1f270857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is completed for you\n",
    "# =======================================================\n",
    "def create_driver(headless=True):\n",
    "    \"\"\"\n",
    "    Installs a chromedriver into the current working directory\n",
    "    and returns an active selenium webdriver.\n",
    "    \"\"\"\n",
    "    # Install chrome driver\n",
    "    driver = chromedriver_autoinstaller.install(cwd=True)\n",
    "    # Create a chrome options object to customize the settings\n",
    "    chrome_options = webdriver.ChromeOptions() \n",
    "    # Set the download path to the current working directory\n",
    "        # (Files will be downloaded in the directory of the notebook\n",
    "        # rather than in the computer's Download folder)\n",
    "    prefs = {'download.default_directory' : os.getcwd()}\n",
    "    chrome_options.add_experimental_option('prefs', prefs)\n",
    "    # Turn off the gui for the web browser\n",
    "    if headless:\n",
    "        chrome_options.add_argument(\"--headless\")\n",
    "    # Create the webdriver object\n",
    "    driver = webdriver.Chrome(driver, \n",
    "                             chrome_options = chrome_options)\n",
    "    return driver\n",
    "# =======================================================\n",
    "\n",
    "def get_department_links(driver):\n",
    "    # Set up the root and url for collecting\n",
    "    # the links for each department's landing page\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    # Connect the webdriver to the url\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    # Collect the html and parse it with BeautifulSoup\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    # Isolate the a tags for the departments\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    # Create the dictionary of department links\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    # Return the department links\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "def get_department_soup(driver, link):\n",
    "    \n",
    "    # Connect the webdriver to the link\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    # Collect the html from the webdriver\n",
    "    # YOUR CODE HERE\n",
    "   \n",
    "    # Parse the html with BeautifulSoup\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    # Return the parsed html\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "# This function is completed for you. \n",
    "# =======================================================\n",
    "# Feel free to read through the comments\n",
    "# That describe the web scraping decisions\n",
    "def get_faculty_divs(program_soup):\n",
    "    # Isolate the accordion\n",
    "    dropdown = program_soup.find('section', {'class': 'accordion'})\n",
    "    \n",
    "    # The biology department organizes\n",
    "    # their web pages in ways that make selecting\n",
    "    # The h2 tag that contains the words \"faculty\" or \"staff\" inadequate.\n",
    "    # Instead, we will select `i` tags that contain white space, but are\n",
    "    # present on every program page. Then we will loop over the `i` tags and \n",
    "    # run a text search for the word faculty/staff on the `h2` tag directly following\n",
    "    # the `i` tag. \n",
    "    i_tags = dropdown.find_all('i')\n",
    "    for tag in i_tags:\n",
    "        if re.findall('STAFF|FACULTY|Faculty|Staff', tag.find_next('h2').text):\n",
    "            # If we have found an h2 tag inside an i tag\n",
    "            # that contains the key words, it is the faculty dropdown!\n",
    "            faculty_drop = tag.find_previous('li')\n",
    "    \n",
    "    # Each faculty member has their own p tag so we will find all p tags\n",
    "    # inside the faculty_drop variable and set that to faculty\n",
    "        # There are some cases where we will end up with \"faculty\"\n",
    "        # that are actually just empty p tags in the html\n",
    "        # This results in a slightly messier dataset, but overall doesn't\n",
    "        # impact the integrity of our data. \n",
    "    faculty = faculty_drop.find('div', {'class': 'content'}).find_all('p')\n",
    "\n",
    "    return faculty\n",
    "# =======================================================\n",
    "\n",
    "def scrape_faculty_data(driver, faculty_divs):\n",
    "    data = []\n",
    "    root = 'https://www.bsc.edu/academics/'\n",
    "    \n",
    "    for person in faculty_divs:\n",
    "        \n",
    "        # Create a default dataset for a staff member\n",
    "        # If we do not find a data point for an instructor\n",
    "        # They still need to have the same number of keys\n",
    "        # If we want to turn this data into a dataframe\n",
    "        default = {\"name\": None,\n",
    "                   \"office\": None,\n",
    "                  \"phone\": None,\n",
    "                  \"email\": None,\n",
    "                  \"classes\": None}\n",
    "        \n",
    "        # Collecting the faculty name turned out to be quite difficult\n",
    "        # The departments deviate from each other pretty dramatically\n",
    "        # in the way they format their faculty dropdown.\n",
    "        # the `.string` attribute was used because it returns\n",
    "        # the strings inside the person div as seperate objects\n",
    "        # Whereas `.text` returns one joined string. This poses a problem\n",
    "        # When faculty members included a bio in the same tag as their name.\n",
    "        name = list(person.strings)\n",
    "        if not name:\n",
    "            # Some p tags were empty. \n",
    "            # This if check ensures we ignore them.\n",
    "            continue\n",
    "        if len(name[0]) > 150:\n",
    "            # If the length of the text is >150, it is likely a bio of some kind\n",
    "            # So we skip this p tag and move to the next\n",
    "            continue\n",
    "        if not name[0].strip():\n",
    "            # If, when we strip the string (remove preceding and trailing spaces)\n",
    "            # The string becomes empty, it is just a ptag filled with white space\n",
    "            try:\n",
    "                # In some cases, the ptag opened with a white space\n",
    "                # with a second nested ptag where the instructors name \n",
    "                # was found. If the first tag was white space\n",
    "                # We are naively setting the name to the second string\n",
    "                # This has been placed inside a try except block in case\n",
    "                # a second string doesn't exist. In which case, \n",
    "                # the ptag is completely\n",
    "                # empty and we jump to the next iteration of ptags\n",
    "                name = name[1]\n",
    "            except:\n",
    "                continue\n",
    "        else:\n",
    "            # If the stripped string is not empty and is not >150\n",
    "            # Then we assume the string is the name of the staff member\n",
    "            name = name[0]\n",
    "        # We set the name data to the scraped string\n",
    "        default['name'] = name\n",
    "        \n",
    "        # Next we check if the ptag contains any `a` tags (links)\n",
    "        if person.find('a'):\n",
    "            if person.find('a').attrs['href'][:2] == '..':\n",
    "            # If an `a` tag was found, we check if the href for the atag\n",
    "            # contains the string '..'\n",
    "            # This is a weird design choice by the web developers\n",
    "            # Where they place '..' at the beginning of relative urls.\n",
    "            # Ultimately, if the url is relative it is most likely the bio page\n",
    "            # for the staff member. \n",
    "                # If a relative path was found\n",
    "                # We collect the href and assemble the url\n",
    "                # for the staff member's bio page \n",
    "                href = person.find('a').attrs['href']\n",
    "                faculty_url = root + href.replace('../', '')\n",
    "                # Next we connect to the web page,\n",
    "                # collect the html, and parse it.\n",
    "                driver.get(faculty_url)\n",
    "                faculty_html = driver.page_source\n",
    "                faculty_soup = BeautifulSoup(faculty_html)\n",
    "                try:\n",
    "                    # The entire next bit is wrapped in a try except\n",
    "                    # This is mostly to keep our code somewhat more readible\n",
    "                    # If we find that we are losing too much data \n",
    "                        # For example if `office` is throwing a lot of errors\n",
    "                        # which means none of the other data points are collected\n",
    "                    # We could consider wrapping each of these datapoints in their\n",
    "                    # own try except block\n",
    "                    # ================= STUDENT WORK =======================\n",
    "                    # Find the office for the staff member\n",
    "                    # YOUR CODE HERE\n",
    "\n",
    "                    # Set the office variable in our default dictionary \n",
    "                    # to the office data\n",
    "                    # YOUR CODE HERE\n",
    "\n",
    "                    # Find the phone number for the staff member\n",
    "                    # YOUR CODE HERE\n",
    "\n",
    "                    # Set the phone variable in our default dictionary \n",
    "                    # to the phone data\n",
    "                    # YOUR CODE HERE\n",
    "\n",
    "                    # Find the email for staff member\n",
    "                    # YOUR CODE HERE\n",
    "\n",
    "                    # Set the email variable in our default dictionary \n",
    "                    # to the email data\n",
    "                    # YOUR CODE HERE\n",
    "\n",
    "                    # Find the classes taught by the staff member\n",
    "                    # YOUR CODE HERE\n",
    "\n",
    "                    # Set the classes variable in our default dictionary \n",
    "                    # to the classes data\n",
    "                    # YOUR CODE HERE\n",
    "                except:\n",
    "                    # If an error is thrown move to the last line in the for loop\n",
    "                    pass\n",
    "            else:\n",
    "                # If no a tag was found, we have a helper function\n",
    "                # parse the information of the staff member in the drop down\n",
    "                    # Some programs have opted to place their entire bio\n",
    "                    # in the dropdown and do not seem to have a seperate web page\n",
    "                phone, email = parse_faculty_no_link(person)\n",
    "                default['phone'] = phone\n",
    "                default['email'] = email\n",
    "        # Append the found data for a staff member to a list\n",
    "        data.append(default)\n",
    "        \n",
    "    # Return data for ever staff member \n",
    "    return data\n",
    "\n",
    "# This function is completed for you. \n",
    "# =======================================================\n",
    "def parse_faculty_no_link(p_tag):\n",
    "    \"\"\"\n",
    "    Helper function for collecting bio data \n",
    "    for staff members who have placed their bio\n",
    "    in the program faculty dropdown\n",
    "    \"\"\"\n",
    "    phone = p_tag.find(text = re.compile('\\(.+\\) \\d{3}-\\d{4}|\\d{3}-\\d{3}-\\d{4}'))\n",
    "    if phone:\n",
    "        phone = phone.strip()\n",
    "    email = p_tag.find(text = re.compile('\\@bsc\\.edu'))\n",
    "    return phone, email\n",
    "# =======================================================\n",
    "\n",
    "def scrape_faculty():\n",
    "    # Create an empty dataframe\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    # Create a webdriver\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    # Collect the links for each department\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    # Loop over the keys of the program links dictionary\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "        # Print the name of the program so we know what\n",
    "        # is happening as our code runs\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        # Collect the link for the program\n",
    "        # by passing in the program name\n",
    "        # to the program links dictionary\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        # Collect the parsed html for the program\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        # Collect the divs for each faculty member\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        # Scrape the data for each faculty member\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        # Turn the scraped data into a dataframe\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        # Add a column called `program` that is set\n",
    "        # to the name of the program we have scraped\n",
    "        # data from\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        # Append the program dataframe to \n",
    "        # the `df` variable that we created before\n",
    "        # the for loop\n",
    "        # YOUR CODE HERE\n",
    "    \n",
    "    # Return df\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be55a7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#__SOLUTION__\n",
    "def create_driver(headless=True):\n",
    "    # Install chrome driver\n",
    "    driver = chromedriver_autoinstaller.install(cwd=True)\n",
    "    # Create a chrome options object to customize the settings\n",
    "    chrome_options = webdriver.ChromeOptions() \n",
    "    # Set the download path to the current working directory\n",
    "        # (Files will be downloaded in the directory of the notebook\n",
    "        # rather than in the computer's Download folder)\n",
    "    prefs = {'download.default_directory' : os.getcwd()}\n",
    "    chrome_options.add_experimental_option('prefs', prefs)\n",
    "    # Turn off the gui for the web browser\n",
    "    if headless:\n",
    "        chrome_options.add_argument(\"--headless\")\n",
    "    # Create the webdriver object\n",
    "    driver = webdriver.Chrome(driver, \n",
    "                             chrome_options = chrome_options)\n",
    "    return driver\n",
    "\n",
    "def get_department_links(driver):\n",
    "    root = 'https://www.bsc.edu/academics/'\n",
    "    url = root + 'index.html'\n",
    "    driver.get(url)\n",
    "    soup = BeautifulSoup(driver.page_source)\n",
    "    div = soup.find_all('h2', text=re.compile('Academic Departments'))[0].parent\n",
    "    a_tags = div.find_all('a')\n",
    "    department_links = {a.text: root + a.attrs['href'] for a in a_tags}\n",
    "    \n",
    "    return department_links\n",
    "\n",
    "def get_department_soup(driver, link):\n",
    "\n",
    "    driver.get(link)\n",
    "    html = driver.page_source\n",
    "   \n",
    "    return BeautifulSoup(html)\n",
    "\n",
    "def get_faculty_divs(program_soup):\n",
    "    # Isolate the accordion\n",
    "    dropdown = program_soup.find('section', {'class': 'accordion'})\n",
    "    \n",
    "    # The biology department organizes\n",
    "    # their web pages in ways that make selecting\n",
    "    # The h2 tag that contains the words \"faculty\" or \"staff\" inadequate.\n",
    "    # Instead, we will select `i` tags that contain white space, but are\n",
    "    # present on every program page. Then we will loop over the `i` tags and \n",
    "    # run a text search for the word faculty/staff on the `h2` tag directly following\n",
    "    # the `i` tag. \n",
    "    i_tags = dropdown.find_all('i')\n",
    "    for tag in i_tags:\n",
    "        if re.findall('STAFF|FACULTY|Faculty|Staff', tag.find_next('h2').text):\n",
    "            # If we have found an h2 tag inside an i tag\n",
    "            # that contains the key words, it is the faculty dropdown!\n",
    "            faculty_drop = tag.find_previous('li')\n",
    "    \n",
    "    # Each faculty member has their own p tag so we will find all p tags\n",
    "    # inside the faculty_drop variable and set that to faculty\n",
    "        # There are some cases where we will end up with \"faculty\"\n",
    "        # that are actually just empty p tags in the html\n",
    "        # This results in a slightly messier dataset, but overall doesn't\n",
    "        # impact the integrity of our data. \n",
    "    faculty = faculty_drop.find('div', {'class': 'content'}).find_all('p')\n",
    "\n",
    "    return faculty\n",
    "\n",
    "\n",
    "def scrape_faculty_data(driver, faculty_divs):\n",
    "    data = []\n",
    "    root = 'https://www.bsc.edu/academics/'\n",
    "    \n",
    "    for person in faculty_divs:\n",
    "        \n",
    "        # Create a default dataset for a staff member\n",
    "        # If we do not find a data point for an instructor\n",
    "        # They still need to have the same number of keys\n",
    "        # If we want to turn this data into a dataframe\n",
    "        default = {\"name\": None,\n",
    "                   \"office\": None,\n",
    "                  \"phone\": None,\n",
    "                  \"email\": None,\n",
    "                  \"classes\": None}\n",
    "        \n",
    "        # Collecting the faculty name turned out to be quite difficult\n",
    "        # The departments deviate from each other pretty dramatically\n",
    "        # in the way they format their faculty dropdown.\n",
    "        # the `.string` attribute was used because it returns\n",
    "        # the strings inside the person div as seperate objects\n",
    "        # Whereas `.text` returns one joined string. This poses a problem\n",
    "        # When faculty members included a bio in the same tag as their name.\n",
    "        name = list(person.strings)\n",
    "        if not name:\n",
    "            # Some p tags were empty. \n",
    "            # This if check ensures we ignore them.\n",
    "            continue\n",
    "        if len(name[0]) > 150:\n",
    "            # If the length of the text is >150, it is likely a bio of some kind\n",
    "            # So we skip this p tag and move to the next\n",
    "            continue\n",
    "        if not name[0].strip():\n",
    "            # If, when we strip the string (remove preceding and trailing spaces)\n",
    "            # The string becomes empty, it is just a ptag filled with white space\n",
    "            try:\n",
    "                # In some cases, the ptag opened with a white space\n",
    "                # with a second nested ptag where the instructors name \n",
    "                # was found. If the first tag was white space\n",
    "                # We are naively setting the name to the second string\n",
    "                # This has been placed inside a try except block in case\n",
    "                # a second string doesn't exist. In which case, \n",
    "                # the ptag is completely\n",
    "                # empty and we jump to the next iteration of ptags\n",
    "                name = name[1]\n",
    "            except:\n",
    "                continue\n",
    "        else:\n",
    "            # If the stripped string is not empty and is not >150\n",
    "            # Then we assume the string is the name of the staff member\n",
    "            name = name[0]\n",
    "        # We set the name data to the scraped string\n",
    "        default['name'] = name\n",
    "        \n",
    "        # Next we check if the ptag contains any `a` tags (links)\n",
    "        if person.find('a'):\n",
    "            if person.find('a').attrs['href'][:2] == '..':\n",
    "            # If an `a` tag was found, we check if the href for the atag\n",
    "            # contains the string '..'\n",
    "            # This is a weird design choice by the web developers\n",
    "            # Where they place '..' at the beginning of relative urls.\n",
    "            # Ultimately, if the url is relative it is most likely the bio page\n",
    "            # for the staff member. \n",
    "                # If a relative path was found\n",
    "                # We collect the href and assemble the url\n",
    "                # for the staff member's bio page \n",
    "                href = person.find('a').attrs['href']\n",
    "                faculty_url = root + href.replace('../', '')\n",
    "                # Next we connect to the web page,\n",
    "                # collect the html, and parse it.\n",
    "                driver.get(faculty_url)\n",
    "                faculty_html = driver.page_source\n",
    "                faculty_soup = BeautifulSoup(faculty_html)\n",
    "                try:\n",
    "                    # The entire next bit is wrapped in a try except\n",
    "                    # This is mostly to keep our code somewhat more readible\n",
    "                    # If we find that we are losing too much data \n",
    "                        # For example if `office` is throwing a lot of errors\n",
    "                        # which means none of the other data points are collected\n",
    "                    # We could consider wrapping each of these datapoints in their\n",
    "                    # own try except block\n",
    "                    office = faculty_soup.find(text = re.compile(\"Office:\"))\n",
    "                    office = office.find_next('p').text\n",
    "                    default['office'] = office\n",
    "                    phone = faculty_soup.find(text = re.compile(\"Office Phone:\"))\n",
    "                    phone = phone.replace(\"Office Phone:\", '').strip()\n",
    "                    default['phone'] = phone\n",
    "                    email = faculty_soup.find(text=re.compile(\"E-mail:\"))\n",
    "                    email = email.find_next('a').attrs['href'].replace('mailto:', '')\n",
    "                    default['email'] = email\n",
    "                    classes = faculty_soup.find(text=re.compile('Courses Taught:'))\n",
    "                    classes = classes.find_next('blockquote').find_all('strong')\n",
    "                    classes = [x.text for x in classes]\n",
    "                    default['classes'] = classes\n",
    "                except:\n",
    "                    # If an error is thrown move to the last line in the for loop\n",
    "                    pass\n",
    "            else:\n",
    "                # If no a tag was found, we have a helper function\n",
    "                # parse the information of the staff member in the drop down\n",
    "                    # Some programs have opted to place their entire bio\n",
    "                    # in the dropdown and do not seem to have a seperate web page\n",
    "                phone, email = parse_faculty_no_link(person)\n",
    "                default['phone'] = phone\n",
    "                default['email'] = email\n",
    "        # Append the found data for a staff member to a list\n",
    "        data.append(default)\n",
    "        \n",
    "    # Return data for ever staff member \n",
    "    return data\n",
    "\n",
    "def parse_faculty_no_link(p_tag):\n",
    "    \"\"\"\n",
    "    Helper function for collecting bio data \n",
    "    for staff members who have placed their bio\n",
    "    in the program faculty dropdown\n",
    "    \"\"\"\n",
    "    phone = p_tag.find(text = re.compile('\\(.+\\) \\d{3}-\\d{4}|\\d{3}-\\d{3}-\\d{4}'))\n",
    "    if phone:\n",
    "        phone = phone.strip()\n",
    "    email = p_tag.find(text = re.compile('\\@bsc\\.edu'))\n",
    "    return phone, email\n",
    "\n",
    "def scrape_faculty():\n",
    "    df = pd.DataFrame()\n",
    "    driver = create_driver()\n",
    "    program_links = get_department_links(driver)\n",
    "\n",
    "    for program in program_links:\n",
    "        print(f'Scraping data for the {program} program.')\n",
    "        link = program_links[program]\n",
    "        soup = get_department_soup(driver, link)\n",
    "        divs = get_faculty_divs(soup)\n",
    "        data = scrape_faculty_data(driver, divs)\n",
    "        frame = pd.DataFrame(data)\n",
    "        frame['program'] = program\n",
    "        df = df.append(frame)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84228e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = scrape_faculty()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b75b59-966e-4bd3-89f3-acef48ca090a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#__SOLUTION__\n",
    "df = scrape_faculty()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff70b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['email'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b028123e-20ce-4cfb-a8d9-13a1c9eaaa77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#__SOLUTION__\n",
    "df.dropna(subset=['email'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415d4aa4",
   "metadata": {},
   "source": [
    "# Senate\n",
    "\n",
    "Below we scrape the data for the current US Senators from the [US Senators Wikipedia Page](https://en.wikipedia.org/wiki/List_of_current_United_States_senators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb00b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the url\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_current_United_States_senators'\n",
    "# Make a request\n",
    "response = get(url)\n",
    "# Collect the html\n",
    "html = response.text\n",
    "# Parse the html\n",
    "soup = BeautifulSoup(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7199b44-a5c9-42a8-94ce-ee9ceebf6913",
   "metadata": {},
   "outputs": [],
   "source": [
    "#__SOLUTION__\n",
    "# Set up the url\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_current_United_States_senators'\n",
    "# Make a request\n",
    "response = get(url)\n",
    "# Collect the html\n",
    "html = response.text\n",
    "# Parse the html\n",
    "soup = BeautifulSoup(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110c95eb-eba7-493b-8afe-9c247f2738fd",
   "metadata": {},
   "source": [
    "First we will look at a really neat web scraping trick with the `pandas` library. \n",
    "\n",
    "First we isolate the `<table>` tag that contains the information about the senators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b0f779",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = soup.find('table', {'id':'senators'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e85e983-ad45-42fb-9708-063d79d7ec88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#__SOLUTION__\n",
    "table = soup.find('table', {'id':'senators'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35525122-bac6-4d4d-89cc-54c239012ea1",
   "metadata": {},
   "source": [
    "Next, we convert the isolated html table to a string datatype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020b4530-ca11-4902-ab05-1d921d6c3eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the soup variable to a string\n",
    "table_string = str(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adf5510-794c-45d2-8cf3-faa78ab056e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#__SOLUTION__\n",
    "# Convert the soup variable to a string\n",
    "table_string = str(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3ad1d5-39fd-4f34-b3ec-fffbc0783b94",
   "metadata": {},
   "source": [
    "And now the cool part!\n",
    "\n",
    "For any `<table>` tag, we can simply pass it into `pd.read_html` and pandas will parse the table for us and return it as a dataframe!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0bf0bd-5d22-4405-a2f8-103c81cf3b5e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "pandas_result = pd.read_html(table_string)\n",
    "pandas_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6315b784-17e0-41cc-8982-04b161d0fd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#__SOLUTION__\n",
    "pandas_result = pd.read_html(table_string)\n",
    "pandas_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1395cbb9-b6c6-48a6-8cbc-80f39352bb2c",
   "metadata": {},
   "source": [
    "The above about looks sort of funny. \n",
    "\n",
    "The developers of pandas made the choice that `read_html` will always about a `list` of dataframes. This did this with the thinking that you could pass `read_html` the html of an entire webpage, and pandas will parse every `<table>` tag found in the html. \n",
    "\n",
    "In this case, we have already isolated the table we want which means the list given to us by `read_html` has a length of `1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67d21d9-f460-4fbe-98b1-6bf9475423ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pandas_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f6910b-0edc-40bd-8f02-dae9ef2c2d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#__SOLUTION__\n",
    "len(pandas_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d805cb-d0fe-4b6f-bd24-6df8ca7d943b",
   "metadata": {},
   "source": [
    "So really, all we need to do is **index** the list returned to us by `read_html`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03dc2bc1-9ba5-4843-88c8-04fb632f619e",
   "metadata": {},
   "outputs": [],
   "source": [
    "senate = pd.read_html(table_string)[0]\n",
    "senate.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae9086a-857c-4087-b33d-bebe474b7064",
   "metadata": {},
   "outputs": [],
   "source": [
    "#__SOLUTION__\n",
    "senate = pd.read_html(table_string)[0]\n",
    "senate.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620e89c2-8be5-46f9-b201-68eca6e33eeb",
   "metadata": {},
   "source": [
    "And we have our data! \n",
    "\n",
    "It is worth noting that the data given to us is a bit messy, and in some cases is missing information we might want. This pandas trick can be really powerful and helpful if we're short on time, but typically a manual scrape will always produce a cleaner result. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d64bc5",
   "metadata": {},
   "source": [
    "## A better scrape:\n",
    "\n",
    "Below, we create a function called `scrape_senate_table` that produced a much cleaner dataframe.\n",
    "\n",
    "This function is completed for you. We encourage you to read through the comments, and even copy the code into a code cell and play around with it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bff3816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell unchanged\n",
    "def scrape_senate_table():\n",
    "    \n",
    "    # Connect to the wikipedia page\n",
    "    response = get('https://en.wikipedia.org/wiki/List_of_current_United_States_senators')\n",
    "    # Collect the html from the page\n",
    "    html = response.text\n",
    "    # Parse the html with BeautifulSoup\n",
    "    soup = BeautifulSoup(html)\n",
    "    # Find the senators table\n",
    "    table = soup.find('table', {'id': 'senators'})\n",
    "    \n",
    "    # The first row in the table contains the column names.\n",
    "    # Isolate the first row, then final all of the column tags.\n",
    "    columns = table.find('tr').find_all('th')\n",
    "    # Collect the row tags for the entire dataset\n",
    "    rows = table.find_all('tr')[1:]\n",
    "    # Create an empty list to append the row data to\n",
    "    senate_data = []\n",
    "    # Create some cleaning functions for text data\n",
    "    remove_new_line = lambda x: x.replace('\\n', '')\n",
    "    split_new_line = lambda x: x.split('\\n')\n",
    "    \n",
    "    # Loop over each row\n",
    "    for row in rows:\n",
    "        # find all td tags from the row\n",
    "        td_tags = row.find_all('td')\n",
    "        # ===============================================================================\n",
    "        # The senators table merges the cell for `state name` so it spans both \n",
    "        # senators from a state. \n",
    "        # When parsing the html, the state name only appears for the senator\n",
    "        # that appears first in the table, and the second appearing senator has \n",
    "        # one less td tag.\n",
    "        # Because of this we need to check the length of the td tags\n",
    "        # If there is one less tag, we add  the state name from the previous iteration\n",
    "        # To the beginning of the list of tags\n",
    "        # ===============================================================================\n",
    "        # Check if the list of tags has the full number of columns\n",
    "        if len(td_tags) == len(columns):\n",
    "            # If it does, store the first element in the list of tags\n",
    "            # to a variable called previous_element\n",
    "            previous_element = td_tags[0]\n",
    "        # If the list of tags does not have the full number of columns\n",
    "        # insert the previous_element variable at the beginning of the list of tags\n",
    "        else:\n",
    "            td_tags.insert(0, previous_element)\n",
    "        \n",
    "        # ===============================================================================\n",
    "        #                              Parse the row data\n",
    "        # ===============================================================================\n",
    "        # Collect the state name\n",
    "        state = remove_new_line(td_tags[0].text)\n",
    "        # Collect the image url\n",
    "        image = td_tags[1].find('img').attrs['src']\n",
    "        # Collect the name of the senator\n",
    "        name = remove_new_line(row.find('th').text)\n",
    "        # Collect the css color string for the political party\n",
    "        party_color = td_tags[2].attrs['style'].split(':')[-1]\n",
    "        # Collect the party name\n",
    "        party_name = remove_new_line(td_tags[3].text)\n",
    "        # Remove the [2] that occasionally follows the word \"Republicans\"\n",
    "        party_name = party_name.split('[')[0]\n",
    "        # Collect the date of birth\n",
    "        dob = ' '.join(remove_new_line(td_tags[4]\\\n",
    "                                       .text)\\\n",
    "                                       .strip()\\\n",
    "                                       .split(' ')[1:4])\n",
    "        # Collect the occupation\n",
    "        occupation = split_new_line(BeautifulSoup(str(td_tags[5])\\\n",
    "                                                  .replace('<br/>', '\\n'))\\\n",
    "                                                  .td\\\n",
    "                                                  .text)\n",
    "        # If only one occupation is present\n",
    "        # Pull that occupation out of the list\n",
    "        # And return a single string\n",
    "        if occupation[1] == '':\n",
    "            occupation = occupation[0]\n",
    "        # Collect the previous office\n",
    "        previous_office = split_new_line(BeautifulSoup(str(td_tags[6])\\\n",
    "                                                       .replace('<br/>', '\\n'))\\\n",
    "                                                       .td\\\n",
    "                                                      .text)\n",
    "        # If only one previous office is present\n",
    "        # Pull that value out of the list\n",
    "        # And return a single string\n",
    "        if previous_office[1] == '':\n",
    "            previous_office = previous_office[0]\n",
    "        # Collect assumed office\n",
    "        assumed_office = remove_new_line(td_tags[7].text)\n",
    "        # Collect the end of their term\n",
    "        term_up = remove_new_line(td_tags[8].text)\n",
    "        # Remove any [number] appendix references\n",
    "        term_up = term_up.split('[')[0]\n",
    "        # Collect their residence\n",
    "        residence = split_new_line(td_tags[9].text)\n",
    "        \n",
    "        # If only one residence is present\n",
    "        # Pull that value out of the list\n",
    "        # And return a single string\n",
    "        if residence[1] == '':\n",
    "            residence = residence[0]\n",
    "            # Many of the residences have a \n",
    "            # link pointing to additional information\n",
    "            # We do not need this\n",
    "            if '[' in residence:\n",
    "                residence = residence[:-3]\n",
    "        # Create data dictionary\n",
    "        collected = {'state': state, 'name': name,'dob': dob, 'party': party_name,\n",
    "                    'party_color': party_color,\n",
    "                    'occupation': occupation,\n",
    "                    'previous_office': previous_office,\n",
    "                    'assumed_office': assumed_office,\n",
    "                    'term_up': term_up, 'residence': residence,\n",
    "                    'portrait': image}\n",
    "        # Append dictionary to the senate_data list\n",
    "        senate_data.append(collected)\n",
    "        \n",
    "    # Once data from all rows has been collected\n",
    "    # return the data as a pandas dataframe\n",
    "    return pd.DataFrame(senate_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8602b6-c148-4ea0-9a06-2638eace6b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#__SOLUTION__\n",
    "# Run this cell unchanged\n",
    "def scrape_senate_table():\n",
    "    \n",
    "    # Connect to the wikipedia page\n",
    "    response = get('https://en.wikipedia.org/wiki/List_of_current_United_States_senators')\n",
    "    # Collect the html from the page\n",
    "    html = response.text\n",
    "    # Parse the html with BeautifulSoup\n",
    "    soup = BeautifulSoup(html)\n",
    "    # Find the senators table\n",
    "    table = soup.find('table', {'id': 'senators'})\n",
    "    \n",
    "    # The first row in the table contains the column names.\n",
    "    # Isolate the first row, then final all of the column tags.\n",
    "    columns = table.find('tr').find_all('th')\n",
    "    # Collect the row tags for the entire dataset\n",
    "    rows = table.find_all('tr')[1:]\n",
    "    # Create an empty list to append the row data to\n",
    "    senate_data = []\n",
    "    # Create some cleaning functions for text data\n",
    "    remove_new_line = lambda x: x.replace('\\n', '')\n",
    "    split_new_line = lambda x: x.split('\\n')\n",
    "    \n",
    "    # Loop over each row\n",
    "    for row in rows:\n",
    "        # find all td tags from the row\n",
    "        td_tags = row.find_all('td')\n",
    "        # ===============================================================================\n",
    "        # The senators table merges the cell for `state name` so it spans both \n",
    "        # senators from a state. \n",
    "        # When parsing the html, the state name only appears for the senator\n",
    "        # that appears first in the table, and the second appearing senator has \n",
    "        # one less td tag.\n",
    "        # Because of this we need to check the length of the td tags\n",
    "        # If there is one less tag, we add  the state name from the previous iteration\n",
    "        # To the beginning of the list of tags\n",
    "        # ===============================================================================\n",
    "        # Check if the list of tags has the full number of columns\n",
    "        if len(td_tags) == len(columns):\n",
    "            # If it does, store the first element in the list of tags\n",
    "            # to a variable called previous_element\n",
    "            previous_element = td_tags[0]\n",
    "        # If the list of tags does not have the full number of columns\n",
    "        # insert the previous_element variable at the beginning of the list of tags\n",
    "        else:\n",
    "            td_tags.insert(0, previous_element)\n",
    "        \n",
    "        # ===============================================================================\n",
    "        #                              Parse the row data\n",
    "        # ===============================================================================\n",
    "        # Collect the state name\n",
    "        state = remove_new_line(td_tags[0].text)\n",
    "        # Collect the image url\n",
    "        image = td_tags[1].find('img').attrs['src']\n",
    "        # Collect the name of the senator\n",
    "        name = remove_new_line(row.find('th').text)\n",
    "        # Collect the css color string for the political party\n",
    "        party_color = td_tags[2].attrs['style'].split(':')[-1]\n",
    "        # Collect the party name\n",
    "        party_name = remove_new_line(td_tags[3].text)\n",
    "        # Remove the [2] that occasionally follows the word \"Republicans\"\n",
    "        party_name = party_name.split('[')[0]\n",
    "        # Collect the date of birth\n",
    "        dob = ' '.join(remove_new_line(td_tags[4]\\\n",
    "                                       .text)\\\n",
    "                                       .strip()\\\n",
    "                                       .split(' ')[1:4])\n",
    "        # Collect the occupation\n",
    "        occupation = split_new_line(BeautifulSoup(str(td_tags[5])\\\n",
    "                                                  .replace('<br/>', '\\n'))\\\n",
    "                                                  .td\\\n",
    "                                                  .text)\n",
    "        # If only one occupation is present\n",
    "        # Pull that occupation out of the list\n",
    "        # And return a single string\n",
    "        if occupation[1] == '':\n",
    "            occupation = occupation[0]\n",
    "        # Collect the previous office\n",
    "        previous_office = split_new_line(BeautifulSoup(str(td_tags[6])\\\n",
    "                                                       .replace('<br/>', '\\n'))\\\n",
    "                                                       .td\\\n",
    "                                                      .text)\n",
    "        # If only one previous office is present\n",
    "        # Pull that value out of the list\n",
    "        # And return a single string\n",
    "        if previous_office[1] == '':\n",
    "            previous_office = previous_office[0]\n",
    "        # Collect assumed office\n",
    "        assumed_office = remove_new_line(td_tags[7].text)\n",
    "        # Collect the end of their term\n",
    "        term_up = remove_new_line(td_tags[8].text)\n",
    "        # Remove any [number] appendix references\n",
    "        term_up = term_up.split('[')[0]\n",
    "        # Collect their residence\n",
    "        residence = split_new_line(td_tags[9].text)\n",
    "        \n",
    "        # If only one residence is present\n",
    "        # Pull that value out of the list\n",
    "        # And return a single string\n",
    "        if residence[1] == '':\n",
    "            residence = residence[0]\n",
    "            # Many of the residences have a \n",
    "            # link pointing to additional information\n",
    "            # We do not need this\n",
    "            if '[' in residence:\n",
    "                residence = residence[:-3]\n",
    "        # Create data dictionary\n",
    "        collected = {'state': state, 'name': name,'dob': dob, 'party': party_name,\n",
    "                    'party_color': party_color,\n",
    "                    'occupation': occupation,\n",
    "                    'previous_office': previous_office,\n",
    "                    'assumed_office': assumed_office,\n",
    "                    'term_up': term_up, 'residence': residence,\n",
    "                    'portrait': image}\n",
    "        # Append dictionary to the senate_data list\n",
    "        senate_data.append(collected)\n",
    "        \n",
    "    # Once data from all rows has been collected\n",
    "    # return the data as a pandas dataframe\n",
    "    return pd.DataFrame(senate_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a53ffc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell unchanged\n",
    "scrape_senate_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb580d83-aba8-4e3c-93ca-88bc414c0943",
   "metadata": {},
   "outputs": [],
   "source": [
    "#__SOLUTION__\n",
    "# Run this cell unchanged\n",
    "scrape_senate_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761819f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
