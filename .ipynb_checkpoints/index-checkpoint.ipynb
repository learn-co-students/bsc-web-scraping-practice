{"cells": [{"cell_type": "markdown", "id": "b6c8c3c7", "metadata": {"index": 0}, "source": ["## Scraping info about the BSC Faculty"]}, {"cell_type": "code", "execution_count": 2, "id": "886f7bce-3bd7-4ebd-be5b-28425fdee68a", "metadata": {"index": 1}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Requirement already satisfied: selenium==3.141.0 in /opt/anaconda3/lib/python3.7/site-packages (from -r requirements.txt (line 1)) (3.141.0)\n", "Requirement already satisfied: chromedriver_autoinstaller==0.2.2 in /opt/anaconda3/lib/python3.7/site-packages (from -r requirements.txt (line 2)) (0.2.2)\n", "Requirement already satisfied: beautifulsoup4==4.9.3 in /opt/anaconda3/lib/python3.7/site-packages (from -r requirements.txt (line 3)) (4.9.3)\n", "Requirement already satisfied: regex==2021.7.6 in /opt/anaconda3/lib/python3.7/site-packages (from -r requirements.txt (line 4)) (2021.7.6)\n", "Requirement already satisfied: urllib3 in /opt/anaconda3/lib/python3.7/site-packages (from selenium==3.141.0->-r requirements.txt (line 1)) (1.26.4)\n", "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/lib/python3.7/site-packages (from beautifulsoup4==4.9.3->-r requirements.txt (line 3)) (2.2.1)\n"]}], "source": ["# Run this cell unchanged\n", "!pip install -r requirements.txt"]}, {"cell_type": "code", "execution_count": 38, "id": "dc95459f", "metadata": {"index": 3}, "outputs": [], "source": ["import os\n", "import re\n", "import pandas as pd\n", "from time import sleep\n", "from requests import get\n", "from bs4 import BeautifulSoup\n", "from selenium import webdriver\n", "import chromedriver_autoinstaller\n", "\n", "\n", "import warnings\n", "warnings.filterwarnings(\"ignore\")\n", "%config Completer.use_jedi = False"]}, {"cell_type": "markdown", "id": "a9665a73-f5a9-4471-aa99-3089a536933d", "metadata": {"index": 5}, "source": ["The goal for today is to scrape the\n", "* Name\n", "* Office \n", "* Office phone\n", "* Email\n", "* The classes taught by the teacher\n", "\n", "For each program that is linked [here](https://www.bsc.edu/academics/index.html)"]}, {"cell_type": "code", "execution_count": 8, "id": "5e82e599", "metadata": {"index": 6}, "outputs": [], "source": ["# Set up the url we want to make requests to. \n", "\n", "# Send request\n", "\n", "# Print the request response\n"]}, {"cell_type": "markdown", "id": "0f4d65b5-9f64-4d36-8b2a-3b1c44c543ef", "metadata": {"index": 8}, "source": ["It looks there are some security measures that are keeping us from making a sucessful request.\n", "\n", "One common way around this is to use a python package called `selenium`.\n", "\n", "Below is some code to set up a selenium webdriver."]}, {"cell_type": "code", "execution_count": 21, "id": "5c779e61", "metadata": {"index": 9}, "outputs": [], "source": ["def create_driver(headless=True):\n", "    driver = chromedriver_autoinstaller.install(cwd=True)\n", "    chrome_options = webdriver.ChromeOptions()     \n", "    if headless:\n", "        chrome_options.add_argument(\"--headless\")\n", "\n", "    driver = webdriver.Chrome(driver, \n", "                             chrome_options = chrome_options)\n", "    return driver\n", "\n", "driver = create_driver()"]}, {"cell_type": "markdown", "id": "eb14610f-aed2-45f0-b059-8b0e675e26b9", "metadata": {"index": 11}, "source": ["Now we can run the code below, and we'll be able to connect to the web page."]}, {"cell_type": "code", "execution_count": 22, "id": "d791395e", "metadata": {"index": 12}, "outputs": [], "source": ["root = 'https://www.bsc.edu/academics/'\n", "url = root + 'index.html'\n", "driver.get(url)"]}, {"cell_type": "markdown", "id": "efa75d42-0d9c-44f7-a2dd-71c3765d36fb", "metadata": {"index": 14}, "source": ["To pull the raw html from our webdriver, we can run the following:"]}, {"cell_type": "code", "execution_count": 23, "id": "a5ec5df0-864e-494e-8ce2-b994e167d927", "metadata": {"index": 15}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["HTML:\n", "<html xmlns=\"http://www.w3.org/1999/xhtml\" class=\"no-js tablesaw-enhanced\" lang=\"en\"><head>\n", "\t\t<meta content=\"IE=edge\" http-equiv=\"X-UA-Compatible\">\n", "\t\t\n"]}], "source": ["html = driver.page_source\n", "\n", "print('HTML:')\n", "print(html[:150])"]}, {"cell_type": "markdown", "id": "1ae5c791-bca8-4163-ad8a-0ddc19d6a454", "metadata": {"index": 17}, "source": ["Next we pass the html into `BeautifulSoup` so we can parse the information embedded inside the html. "]}, {"cell_type": "code", "execution_count": 24, "id": "5eecccaa", "metadata": {"index": 18}, "outputs": [], "source": ["# Pass html into BeautifulSoup\n", "soup = BeautifulSoup(html)"]}, {"cell_type": "markdown", "id": "96d985a8-f102-4171-828c-2ceffb9fcea5", "metadata": {"index": 20}, "source": ["Ok now let's start scraping. \n", "\n", "For this page, we need to collect the name of the department and the url for each department's landing page. \n", "\n", "Let's create a dictionary that has the following format:\n", "\n", "```\n", "{department_name: deparment_url}\n", "```"]}, {"cell_type": "code", "execution_count": null, "id": "0ab2642d-957b-4fc4-bf8c-4a1c76c7b6d1", "metadata": {"index": 21}, "outputs": [], "source": ["# Your code here"]}, {"cell_type": "markdown", "id": "086cd809-f913-4411-a4ce-61f4cb30b854", "metadata": {"index": 23}, "source": ["Next, we need to collect the html for each of the department landing pages\n", "\n", "Let's create a dictionary with the following format:\n", "\n", "```\n", "{department_name: department_html}\n", "```"]}, {"cell_type": "code", "execution_count": null, "id": "93eb8f28-e73c-4e17-a17d-829f523e253d", "metadata": {"index": 24}, "outputs": [], "source": ["# Your code here"]}, {"cell_type": "markdown", "id": "4cadef6a-a159-4205-84e9-7d3f77c5e290", "metadata": {"index": 26}, "source": ["Now let's create a dictionary containing each departments `soup` (or their parsed html files)\n", "\n", "```\n", "{department_name: department_soup}\n", "```"]}, {"cell_type": "code", "execution_count": 28, "id": "14c1d52d-bfa0-495d-b1b3-d23f0d5420f1", "metadata": {"index": 27}, "outputs": [], "source": ["# Your code here"]}, {"cell_type": "markdown", "id": "8b83a89b-7f89-462e-a31b-2ed1beb68a06", "metadata": {"index": 29}, "source": ["Ok. So at this point, we have successfully collect the parsed html for each department's landing page. \n", "\n", "Now, we need to figure out how to write code that will scrape the information for each department's landing page. The problem we will discover is that none of the landing pages are formatted exactly the same. *This* is where the true messying of web scraping appears. \n", "\n", "In the cell below, let's isolate the soup for the `Sociology` department. "]}, {"cell_type": "code", "execution_count": 30, "id": "72bbd9a5-db20-42e2-b0ad-605ddcdfd697", "metadata": {"index": 30}, "outputs": [], "source": ["program = department_soup['Sociology']"]}, {"cell_type": "markdown", "id": "dc8162ff-9f2d-43c5-a09b-7eb163d28e94", "metadata": {"index": 32}, "source": ["Now we need to isolate the section of the html that contains the information we want."]}, {"cell_type": "code", "execution_count": 31, "id": "1e2cda7d-34d5-4887-9283-2431202f3a2a", "metadata": {"index": 33}, "outputs": [], "source": ["# Your code here"]}, {"cell_type": "markdown", "id": "7844e7e3-62e7-41bc-9f2d-284511cc6520", "metadata": {"index": 35}, "source": ["Ok. This is where is gets really hairy. \n", "\n", "We need to:\n", "* Loop over the tags inside the `faculty` variable\n", "* Collect the name of the staff member\n", "* Check if the staff member has a link for a bio\n", "    * If they do collect the url for that page, collect html and parse it\n", "    * Scrape office, phone, email, and classes from the bio\n", "* If not, see if we can find any of the data points\n", "* Append the data we have scraped for a staff member to a list"]}, {"cell_type": "code", "execution_count": 45, "id": "18a00223-514c-46e9-9a6e-ce251009ab6e", "metadata": {"index": 36}, "outputs": [], "source": ["# We are going to define a function for collecting the data \n", "# for faculty members that do not have a bio page\n", "def parse_faculty_no_link(p_tag):\n", "    \"\"\"\n", "    Helper function for collecting bio data \n", "    for staff members who have placed their bio\n", "    in the program faculty dropdown\n", "    \"\"\"\n", "    phone = p_tag.find(text = re.compile('\\(.+\\) \\d{3}-\\d{4}|\\d{3}-\\d{3}-\\d{4}'))\n", "    if phone:\n", "        phone = phone.strip()\n", "    email = p_tag.find(text = re.compile('\\@bsc\\.edu'))\n", "    return phone, email\n", "\n", " \n", "\n", "# Now let's write code collect the data for every faculty member\n", "# in a department!\n", "\n", "data = []\n", "root = 'https://www.bsc.edu/academics/'\n", "\n", "# Loop over the tags in faculty\n", "for person in faculty:\n", "\n", "    # Create a default dataset for a staff member\n", "    # that sets the default values to None\n", "    # That way, if we do not find anything, \n", "    # we have a null value for the observation\n", "    default = {\"name\": None,\n", "               \"office\": None,\n", "              \"phone\": None,\n", "              \"email\": None,\n", "              \"classes\": None}\n", "\n", "    # Collecting the faculty name turned out to be quite difficult\n", "    # The departments deviate from each other pretty dramatically\n", "    # in the way they format their faculty dropdown.\n", "    # the `.string` attribute was used because it returns\n", "    # the strings inside the person div as seperate objects\n", "    # Whereas `.text` returns one joined string. This poses a problem\n", "    # When faculty members included a bio in the same tag as their name.\n", "    name = list(person.strings)\n", "    if not name:\n", "        # Some p tags were empty. \n", "        # This if check ensures we ignore them.\n", "        continue\n", "    if len(name[0]) > 150:\n", "        # If the length of the text is >150, it is likely a bio of some kind\n", "        # So we skip this p tag and move to the next\n", "        continue\n", "    if not name[0].strip():\n", "        # If, when we strip the string (remove preceding and trailing spaces)\n", "        # The string becomes empty, it is just a ptag filled with white space\n", "        try:\n", "            # In some cases, the ptag opened with a white space\n", "            # with a second nested ptag where the instructors name \n", "            # was found. If the first tag was white space\n", "            # We are naively setting the name to the second string\n", "            # This has been placed inside a try except block in case\n", "            # a second string doesn't exist. In which case, \n", "            # the ptag is completely\n", "            # empty and we jump to the next iteration of ptags\n", "            name = name[1]\n", "        except:\n", "            continue\n", "    else:\n", "        # If the stripped string is not empty and is not >150\n", "        # Then we assume the string is the name of the staff member\n", "        name = name[0]\n", "    # We set the name data to the scraped string\n", "    default['name'] = name\n", "\n", "    # Next we check if the ptag contains any `a` tags (links)\n", "    if person.find('a'):\n", "        if person.find('a').attrs['href'][:2] == '..':\n", "        # If an `a` tag was found, we check if the href for the atag\n", "        # contains the string '..'\n", "        # This is a weird design choice by the web developers\n", "        # Where they place '..' at the beginning of relative urls.\n", "        # Ultimately, if the url is relative it is most likely the bio page\n", "        # for the staff member. \n", "            # If a relative path was found\n", "            # We collect the href and assemble the url\n", "            # for the staff member's bio page \n", "            href = person.find('a').attrs['href']\n", "            faculty_url = root + href.replace('../', '')\n", "            # Next we connect to the web page,\n", "            # collect the html, and parse it.\n", "            driver.get(faculty_url)\n", "            faculty_html = driver.page_source\n", "            faculty_soup = BeautifulSoup(faculty_html)\n", "            try:\n", "                pass\n", "                # The entire next bit is wrapped in a try except\n", "                # This is mostly to keep our code somewhat more readible\n", "                # If we find that we are losing too much data \n", "                    # For example if `office` is throwing a lot of errors\n", "                    # which means none of the other data points are collected\n", "                # We could consider wrapping each of these datapoints in their\n", "                # own try except block\n", "                \n", "                # ================= STUDENT WORK =======================\n", "                # Find the office for the staff member\n", "                # YOUR CODE HERE\n", "                \n", "                # Set the office variable in our default dictionary \n", "                # to the office data\n", "                # YOUR CODE HERE\n", "                \n", "                # Find the phone number for the staff member\n", "                # YOUR CODE HERE\n", "                \n", "                # Set the phone variable in our default dictionary \n", "                # to the phone data\n", "                # YOUR CODE HERE\n", "                \n", "                # Find the email for staff member\n", "                # YOUR CODE HERE\n", "                \n", "                # Set the email variable in our default dictionary \n", "                # to the email data\n", "                # YOUR CODE HERE\n", "                \n", "                # Find the classes taught by the staff member\n", "                # YOUR CODE HERE\n", "                \n", "                # Set the classes variable in our default dictionary \n", "                # to the classes data\n", "                # YOUR CODE HERE\n", "                \n", "            except:\n", "                # If an error is thrown move to the last line in the for loop\n", "                pass\n", "        else:\n", "            # If no a tag was found, we have a helper function\n", "            # parse the information of the staff member in the drop down\n", "                # Some programs have opted to place their entire bio\n", "                # in the dropdown and do not seem to have a seperate web page\n", "            phone, email = parse_faculty_no_link(person)\n", "            default['phone'] = phone\n", "            default['email'] = email\n", "    # Append the found data for a staff member to a list\n", "    data.append(default)"]}, {"cell_type": "markdown", "id": "3c4dc79d-ad60-4ade-8365-41b3741df2ff", "metadata": {"index": 37}, "source": ["Let's take a look at the results..."]}, {"cell_type": "code", "execution_count": 47, "id": "4d5f42c4-0114-40d0-9cfa-a0fd35a6fcac", "metadata": {"index": 38}, "outputs": [{"data": {"text/plain": ["[{'name': 'William Holt',\n", "  'office': '313 Harbert',\n", "  'phone': '(205) 226-4834',\n", "  'email': 'wholt@bsc.edu',\n", "  'classes': ['UES 110 Sustainability: Southern Cities & The World',\n", "   'UES 210 Environmental Problems and Policy (1)',\n", "   'SO 373 Urban Sociology (1)',\n", "   'SO 376 Environmental Sociology (1)',\n", "   'UES 470 Senior Seminar in Environmental Studies (1)']},\n", " {'name': 'Meghan Mills',\n", "  'office': 'Harbert 213',\n", "  'phone': '(205) 226-4791',\n", "  'email': 'mmills@bsc.edu',\n", "  'classes': ['SO 101 Introduction to Sociology',\n", "   'SO293 Health and Society',\n", "   'SO370 Sociology of Medicine']},\n", " {'name': 'Stephanie Hansard',\n", "  'office': None,\n", "  'phone': None,\n", "  'email': None,\n", "  'classes': None},\n", " {'name': 'Katie McIntyre',\n", "  'office': None,\n", "  'phone': None,\n", "  'email': None,\n", "  'classes': None}]"]}, "execution_count": 47, "metadata": {}, "output_type": "execute_result"}], "source": ["data"]}, {"cell_type": "markdown", "id": "65a00eb6-1dee-4ab3-a3f4-3e391a677ef5", "metadata": {"index": 39}, "source": ["This looks good. Now let's put it all together!\n", "\n", "To make our web scraping code *reusable*, which I an extremely important part of writing powerful web scraping, we need to toss this entire process into functions. \n", "\n", "**Please create a functions below:**"]}, {"cell_type": "code", "execution_count": null, "id": "3e9c5055-e7b1-4df9-81b1-b46d1f270857", "metadata": {"index": 40}, "outputs": [], "source": ["# This function is completed for you\n", "# =======================================================\n", "def create_driver(headless=True):\n", "    \"\"\"\n", "    Installs a chromedriver into the current working directory\n", "    and returns an active selenium webdriver.\n", "    \"\"\"\n", "    # Install chrome driver\n", "    driver = chromedriver_autoinstaller.install(cwd=True)\n", "    # Create a chrome options object to customize the settings\n", "    chrome_options = webdriver.ChromeOptions() \n", "    # Set the download path to the current working directory\n", "        # (Files will be downloaded in the directory of the notebook\n", "        # rather than in the computer's Download folder)\n", "    prefs = {'download.default_directory' : os.getcwd()}\n", "    chrome_options.add_experimental_option('prefs', prefs)\n", "    # Turn off the gui for the web browser\n", "    if headless:\n", "        chrome_options.add_argument(\"--headless\")\n", "    # Create the webdriver object\n", "    driver = webdriver.Chrome(driver, \n", "                             chrome_options = chrome_options)\n", "    return driver\n", "# =======================================================\n", "\n", "def get_department_links(driver):\n", "    # Set up the root and url for collecting\n", "    # the links for each department's landing page\n", "    # YOUR CODE HERE\n", "    \n", "    # Connect the webdriver to the url\n", "    # YOUR CODE HERE\n", "    \n", "    # Collect the html and parse it with BeautifulSoup\n", "    # YOUR CODE HERE\n", "    \n", "    # Isolate the a tags for the departments\n", "    # YOUR CODE HERE\n", "    \n", "    # Create the dictionary of department links\n", "    # YOUR CODE HERE\n", "    \n", "    # Return the department links\n", "    # YOUR CODE HERE\n", "\n", "def get_department_soup(driver, link):\n", "    \n", "    # Connect the webdriver to the link\n", "    # YOUR CODE HERE\n", "    \n", "    # Collect the html from the webdriver\n", "    # YOUR CODE HERE\n", "   \n", "    # Parse the html with BeautifulSoup\n", "    # YOUR CODE HERE\n", "    \n", "    # Return the parsed html\n", "    # YOUR CODE HERE\n", "\n", "# This function is completed for you. \n", "# =======================================================\n", "# Feel free to read through the comments\n", "# That describe the web scraping decisions\n", "def get_faculty_divs(program_soup):\n", "    # Isolate the accordion\n", "    dropdown = program_soup.find('section', {'class': 'accordion'})\n", "    \n", "    # The biology department organizes\n", "    # their web pages in ways that make selecting\n", "    # The h2 tag that contains the words \"faculty\" or \"staff\" inadequate.\n", "    # Instead, we will select `i` tags that contain white space, but are\n", "    # present on every program page. Then we will loop over the `i` tags and \n", "    # run a text search for the word faculty/staff on the `h2` tag directly following\n", "    # the `i` tag. \n", "    i_tags = dropdown.find_all('i')\n", "    for tag in i_tags:\n", "        if re.findall('STAFF|FACULTY|Faculty|Staff', tag.find_next('h2').text):\n", "            # If we have found an h2 tag inside an i tag\n", "            # that contains the key words, it is the faculty dropdown!\n", "            faculty_drop = tag.find_previous('li')\n", "    \n", "    # Each faculty member has their own p tag so we will find all p tags\n", "    # inside the faculty_drop variable and set that to faculty\n", "        # There are some cases where we will end up with \"faculty\"\n", "        # that are actually just empty p tags in the html\n", "        # This results in a slightly messier dataset, but overall doesn't\n", "        # impact the integrity of our data. \n", "    faculty = faculty_drop.find('div', {'class': 'content'}).find_all('p')\n", "\n", "    return faculty\n", "# =======================================================\n", "\n", "def scrape_faculty_data(driver, faculty_divs):\n", "    data = []\n", "    root = 'https://www.bsc.edu/academics/'\n", "    \n", "    for person in faculty_divs:\n", "        \n", "        # Create a default dataset for a staff member\n", "        # If we do not find a data point for an instructor\n", "        # They still need to have the same number of keys\n", "        # If we want to turn this data into a dataframe\n", "        default = {\"name\": None,\n", "                   \"office\": None,\n", "                  \"phone\": None,\n", "                  \"email\": None,\n", "                  \"classes\": None}\n", "        \n", "        # Collecting the faculty name turned out to be quite difficult\n", "        # The departments deviate from each other pretty dramatically\n", "        # in the way they format their faculty dropdown.\n", "        # the `.string` attribute was used because it returns\n", "        # the strings inside the person div as seperate objects\n", "        # Whereas `.text` returns one joined string. This poses a problem\n", "        # When faculty members included a bio in the same tag as their name.\n", "        name = list(person.strings)\n", "        if not name:\n", "            # Some p tags were empty. \n", "            # This if check ensures we ignore them.\n", "            continue\n", "        if len(name[0]) > 150:\n", "            # If the length of the text is >150, it is likely a bio of some kind\n", "            # So we skip this p tag and move to the next\n", "            continue\n", "        if not name[0].strip():\n", "            # If, when we strip the string (remove preceding and trailing spaces)\n", "            # The string becomes empty, it is just a ptag filled with white space\n", "            try:\n", "                # In some cases, the ptag opened with a white space\n", "                # with a second nested ptag where the instructors name \n", "                # was found. If the first tag was white space\n", "                # We are naively setting the name to the second string\n", "                # This has been placed inside a try except block in case\n", "                # a second string doesn't exist. In which case, \n", "                # the ptag is completely\n", "                # empty and we jump to the next iteration of ptags\n", "                name = name[1]\n", "            except:\n", "                continue\n", "        else:\n", "            # If the stripped string is not empty and is not >150\n", "            # Then we assume the string is the name of the staff member\n", "            name = name[0]\n", "        # We set the name data to the scraped string\n", "        default['name'] = name\n", "        \n", "        # Next we check if the ptag contains any `a` tags (links)\n", "        if person.find('a'):\n", "            if person.find('a').attrs['href'][:2] == '..':\n", "            # If an `a` tag was found, we check if the href for the atag\n", "            # contains the string '..'\n", "            # This is a weird design choice by the web developers\n", "            # Where they place '..' at the beginning of relative urls.\n", "            # Ultimately, if the url is relative it is most likely the bio page\n", "            # for the staff member. \n", "                # If a relative path was found\n", "                # We collect the href and assemble the url\n", "                # for the staff member's bio page \n", "                href = person.find('a').attrs['href']\n", "                faculty_url = root + href.replace('../', '')\n", "                # Next we connect to the web page,\n", "                # collect the html, and parse it.\n", "                driver.get(faculty_url)\n", "                faculty_html = driver.page_source\n", "                faculty_soup = BeautifulSoup(faculty_html)\n", "                try:\n", "                    # The entire next bit is wrapped in a try except\n", "                    # This is mostly to keep our code somewhat more readible\n", "                    # If we find that we are losing too much data \n", "                        # For example if `office` is throwing a lot of errors\n", "                        # which means none of the other data points are collected\n", "                    # We could consider wrapping each of these datapoints in their\n", "                    # own try except block\n", "                    # ================= STUDENT WORK =======================\n", "                    # Find the office for the staff member\n", "                    # YOUR CODE HERE\n", "\n", "                    # Set the office variable in our default dictionary \n", "                    # to the office data\n", "                    # YOUR CODE HERE\n", "\n", "                    # Find the phone number for the staff member\n", "                    # YOUR CODE HERE\n", "\n", "                    # Set the phone variable in our default dictionary \n", "                    # to the phone data\n", "                    # YOUR CODE HERE\n", "\n", "                    # Find the email for staff member\n", "                    # YOUR CODE HERE\n", "\n", "                    # Set the email variable in our default dictionary \n", "                    # to the email data\n", "                    # YOUR CODE HERE\n", "\n", "                    # Find the classes taught by the staff member\n", "                    # YOUR CODE HERE\n", "\n", "                    # Set the classes variable in our default dictionary \n", "                    # to the classes data\n", "                    # YOUR CODE HERE\n", "                except:\n", "                    # If an error is thrown move to the last line in the for loop\n", "                    pass\n", "            else:\n", "                # If no a tag was found, we have a helper function\n", "                # parse the information of the staff member in the drop down\n", "                    # Some programs have opted to place their entire bio\n", "                    # in the dropdown and do not seem to have a seperate web page\n", "                phone, email = parse_faculty_no_link(person)\n", "                default['phone'] = phone\n", "                default['email'] = email\n", "        # Append the found data for a staff member to a list\n", "        data.append(default)\n", "        \n", "    # Return data for ever staff member \n", "    return data\n", "\n", "# This function is completed for you. \n", "# =======================================================\n", "def parse_faculty_no_link(p_tag):\n", "    \"\"\"\n", "    Helper function for collecting bio data \n", "    for staff members who have placed their bio\n", "    in the program faculty dropdown\n", "    \"\"\"\n", "    phone = p_tag.find(text = re.compile('\\(.+\\) \\d{3}-\\d{4}|\\d{3}-\\d{3}-\\d{4}'))\n", "    if phone:\n", "        phone = phone.strip()\n", "    email = p_tag.find(text = re.compile('\\@bsc\\.edu'))\n", "    return phone, email\n", "# =======================================================\n", "\n", "def scrape_faculty():\n", "    # Create an empty dataframe\n", "    # YOUR CODE HERE\n", "    \n", "    # Create a webdriver\n", "    # YOUR CODE HERE\n", "    \n", "    # Collect the links for each department\n", "    # YOUR CODE HERE\n", "\n", "    # Loop over the keys of the program links dictionary\n", "    # YOUR CODE HERE\n", "    \n", "        # Print the name of the program so we know what\n", "        # is happening as our code runs\n", "        # YOUR CODE HERE\n", "        \n", "        # Collect the link for the program\n", "        # by passing in the program name\n", "        # to the program links dictionary\n", "        # YOUR CODE HERE\n", "        \n", "        # Collect the parsed html for the program\n", "        # YOUR CODE HERE\n", "        \n", "        # Collect the divs for each faculty member\n", "        # YOUR CODE HERE\n", "        \n", "        # Scrape the data for each faculty member\n", "        # YOUR CODE HERE\n", "        \n", "        # Turn the scraped data into a dataframe\n", "        # YOUR CODE HERE\n", "        \n", "        # Add a column called `program` that is set\n", "        # to the name of the program we have scraped\n", "        # data from\n", "        # YOUR CODE HERE\n", "        \n", "        # Append the program dataframe to \n", "        # the `df` variable that we created before\n", "        # the for loop\n", "        # YOUR CODE HERE\n", "    \n", "    # Return df\n", "    return df"]}, {"cell_type": "code", "execution_count": 50, "id": "84228e38", "metadata": {"index": 42}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Scraping data for the Applied Computer Science program.\n", "Scraping data for the Mathematics program.\n", "Scraping data for the Art & Art History program.\n", "Scraping data for the Media And Film Studies program.\n", "Scraping data for the Biology program.\n", "Scraping data for the Music program.\n", "Scraping data for the Business program.\n", "Scraping data for the Philosophy program.\n", "Scraping data for the Chemistry program.\n", "Scraping data for the Political Science program.\n", "Scraping data for the Classics program.\n", "Scraping data for the Psychology program.\n", "Scraping data for the Economics program.\n", "Scraping data for the Physics program.\n", "Scraping data for the Education program.\n", "Scraping data for the Religion program.\n", "Scraping data for the English program.\n", "Scraping data for the Sociology program.\n", "Scraping data for the Foreign Languages program.\n", "Scraping data for the Theatre program.\n", "Scraping data for the History program.\n", "Scraping data for the Urban Environmental Studies program.\n"]}], "source": ["df = scrape_faculty()"]}, {"cell_type": "code", "execution_count": 51, "id": "0ff70b7e", "metadata": {"index": 44}, "outputs": [{"data": {"text/html": ["<div>\n", "<style scoped>\n", "    .dataframe tbody tr th:only-of-type {\n", "        vertical-align: middle;\n", "    }\n", "\n", "    .dataframe tbody tr th {\n", "        vertical-align: top;\n", "    }\n", "\n", "    .dataframe thead th {\n", "        text-align: right;\n", "    }\n", "</style>\n", "<table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", "    <tr style=\"text-align: right;\">\n", "      <th></th>\n", "      <th>name</th>\n", "      <th>office</th>\n", "      <th>phone</th>\n", "      <th>email</th>\n", "      <th>classes</th>\n", "      <th>program</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <th>0</th>\n", "      <td>Dr. Amber Wagner</td>\n", "      <td>None</td>\n", "      <td>None</td>\n", "      <td>anwagner@bsc.edu</td>\n", "      <td>None</td>\n", "      <td>Applied Computer Science</td>\n", "    </tr>\n", "    <tr>\n", "      <th>1</th>\n", "      <td>Dr. Anthony Winchester</td>\n", "      <td>None</td>\n", "      <td>None</td>\n", "      <td>agwinche@bsc.edu</td>\n", "      <td>None</td>\n", "      <td>Applied Computer Science</td>\n", "    </tr>\n", "    <tr>\n", "      <th>0</th>\n", "      <td>Jeffrey Barton</td>\n", "      <td>111 Olin Center</td>\n", "      <td>(205) 226-3027</td>\n", "      <td>jbarton@bsc.edu</td>\n", "      <td>[]</td>\n", "      <td>Mathematics</td>\n", "    </tr>\n", "    <tr>\n", "      <th>1</th>\n", "      <td>Catherine Cashio</td>\n", "      <td>Olin 109</td>\n", "      <td>(205) 226-3025</td>\n", "      <td>clcashio@bsc.edu</td>\n", "      <td>[MA 207 General Statistics (1), MA 455 Introdu...</td>\n", "      <td>Mathematics</td>\n", "    </tr>\n", "    <tr>\n", "      <th>2</th>\n", "      <td>E-mail:</td>\n", "      <td>None</td>\n", "      <td>None</td>\n", "      <td>clcashio@bsc.edu</td>\n", "      <td>None</td>\n", "      <td>Mathematics</td>\n", "    </tr>\n", "    <tr>\n", "      <th>...</th>\n", "      <td>...</td>\n", "      <td>...</td>\n", "      <td>...</td>\n", "      <td>...</td>\n", "      <td>...</td>\n", "      <td>...</td>\n", "    </tr>\n", "    <tr>\n", "      <th>8</th>\n", "      <td>Megan Gibbons</td>\n", "      <td>Stephens Science Center 236</td>\n", "      <td>(205) 226-4874</td>\n", "      <td>mgibbons@bsc.edu</td>\n", "      <td>[BI 115 Organismal Biology (1), BI 225 Evoluti...</td>\n", "      <td>Urban Environmental Studies</td>\n", "    </tr>\n", "    <tr>\n", "      <th>9</th>\n", "      <td>Bill Myers</td>\n", "      <td>HC 222</td>\n", "      <td>(205) 226-4868</td>\n", "      <td>bmyers@bsc.edu</td>\n", "      <td>[]</td>\n", "      <td>Urban Environmental Studies</td>\n", "    </tr>\n", "    <tr>\n", "      <th>10</th>\n", "      <td>Rebekah Parker</td>\n", "      <td>None</td>\n", "      <td>None</td>\n", "      <td>rpparker@bsc.edu</td>\n", "      <td>None</td>\n", "      <td>Urban Environmental Studies</td>\n", "    </tr>\n", "    <tr>\n", "      <th>11</th>\n", "      <td>Kathleen Greer Rossmann</td>\n", "      <td>Harbert 309</td>\n", "      <td>(205) 226-4603</td>\n", "      <td>KRossman@bsc.edu</td>\n", "      <td>[]</td>\n", "      <td>Urban Environmental Studies</td>\n", "    </tr>\n", "    <tr>\n", "      <th>12</th>\n", "      <td>Pete Van Zandt</td>\n", "      <td>Stephens Science Center 228</td>\n", "      <td>(205) 226-7817</td>\n", "      <td>pvanzand@bsc.edu</td>\n", "      <td>[BI 101 Explorations in Biology\u00a0, BI 206 Field...</td>\n", "      <td>Urban Environmental Studies</td>\n", "    </tr>\n", "  </tbody>\n", "</table>\n", "<p>150 rows \u00d7 6 columns</p>\n", "</div>"], "text/plain": ["                       name                       office           phone  \\\n", "0         Dr. Amber Wagner                          None            None   \n", "1   Dr. Anthony Winchester                          None            None   \n", "0            Jeffrey Barton              111 Olin Center  (205) 226-3027   \n", "1          Catherine Cashio                     Olin 109  (205) 226-3025   \n", "2                   E-mail:                         None            None   \n", "..                      ...                          ...             ...   \n", "8             Megan Gibbons  Stephens Science Center 236  (205) 226-4874   \n", "9                Bill Myers                       HC 222  (205) 226-4868   \n", "10           Rebekah Parker                         None            None   \n", "11  Kathleen Greer Rossmann                  Harbert 309  (205) 226-4603   \n", "12           Pete Van Zandt  Stephens Science Center 228  (205) 226-7817   \n", "\n", "               email                                            classes  \\\n", "0   anwagner@bsc.edu                                               None   \n", "1   agwinche@bsc.edu                                               None   \n", "0    jbarton@bsc.edu                                                 []   \n", "1   clcashio@bsc.edu  [MA 207 General Statistics (1), MA 455 Introdu...   \n", "2   clcashio@bsc.edu                                               None   \n", "..               ...                                                ...   \n", "8   mgibbons@bsc.edu  [BI 115 Organismal Biology (1), BI 225 Evoluti...   \n", "9     bmyers@bsc.edu                                                 []   \n", "10  rpparker@bsc.edu                                               None   \n", "11  KRossman@bsc.edu                                                 []   \n", "12  pvanzand@bsc.edu  [BI 101 Explorations in Biology\u00a0, BI 206 Field...   \n", "\n", "                        program  \n", "0      Applied Computer Science  \n", "1      Applied Computer Science  \n", "0                   Mathematics  \n", "1                   Mathematics  \n", "2                   Mathematics  \n", "..                          ...  \n", "8   Urban Environmental Studies  \n", "9   Urban Environmental Studies  \n", "10  Urban Environmental Studies  \n", "11  Urban Environmental Studies  \n", "12  Urban Environmental Studies  \n", "\n", "[150 rows x 6 columns]"]}, "execution_count": 51, "metadata": {}, "output_type": "execute_result"}], "source": ["df.dropna(subset=['email'])"]}, {"cell_type": "markdown", "id": "415d4aa4", "metadata": {"index": 46}, "source": ["# Senate\n", "\n", "Below we scrape the data for the current US Senators from the [US Senators Wikipedia Page](https://en.wikipedia.org/wiki/List_of_current_United_States_senators)"]}, {"cell_type": "code", "execution_count": 52, "id": "beb00b20", "metadata": {"index": 47}, "outputs": [], "source": ["# Set up the url\n", "url = 'https://en.wikipedia.org/wiki/List_of_current_United_States_senators'\n", "# Make a request\n", "response = get(url)\n", "# Collect the html\n", "html = response.text\n", "# Parse the html\n", "soup = BeautifulSoup(html)"]}, {"cell_type": "markdown", "id": "110c95eb-eba7-493b-8afe-9c247f2738fd", "metadata": {"index": 49}, "source": ["First we will look at a really neat web scraping trick with the `pandas` library. \n", "\n", "First we isolate the `<table>` tag that contains the information about the senators."]}, {"cell_type": "code", "execution_count": 56, "id": "33b0f779", "metadata": {"index": 50}, "outputs": [], "source": ["table = soup.find('table', {'id':'senators'})"]}, {"cell_type": "markdown", "id": "35525122-bac6-4d4d-89cc-54c239012ea1", "metadata": {"index": 52}, "source": ["Next, we convert the isolated html table to a string datatype."]}, {"cell_type": "code", "execution_count": 57, "id": "020b4530-ca11-4902-ab05-1d921d6c3eca", "metadata": {"index": 53}, "outputs": [], "source": ["# Convert the soup variable to a string\n", "table_string = str(table)"]}, {"cell_type": "markdown", "id": "9d3ad1d5-39fd-4f34-b3ec-fffbc0783b94", "metadata": {"index": 55}, "source": ["And now the cool part!\n", "\n", "For any `<table>` tag, we can simply pass it into `pd.read_html` and pandas will parse the table for us and return it as a dataframe!"]}, {"cell_type": "code", "execution_count": 62, "id": "6a0bf0bd-5d22-4405-a2f8-103c81cf3b5e", "metadata": {"scrolled": true, "tags": [], "index": 56}, "outputs": [{"data": {"text/plain": ["[            State  Portrait               Senator  Party        Party.1  \\\n", " 0         Alabama       NaN        Richard Shelby    NaN  Republican[2]   \n", " 1         Alabama       NaN      Tommy Tuberville    NaN     Republican   \n", " 2          Alaska       NaN        Lisa Murkowski    NaN     Republican   \n", " 3          Alaska       NaN          Dan Sullivan    NaN     Republican   \n", " 4         Arizona       NaN        Kyrsten Sinema    NaN     Democratic   \n", " ..            ...       ...                   ...    ...            ...   \n", " 95  West Virginia       NaN  Shelley Moore Capito    NaN     Republican   \n", " 96      Wisconsin       NaN           Ron Johnson    NaN     Republican   \n", " 97      Wisconsin       NaN         Tammy Baldwin    NaN     Democratic   \n", " 98        Wyoming       NaN         John Barrasso    NaN     Republican   \n", " 99        Wyoming       NaN        Cynthia Lummis    NaN     Republican   \n", " \n", "         Born                                      Occupation(s)  \\\n", " 0   (age\u00a087)                                             Lawyer   \n", " 1   (age\u00a066)  College football coachPartner, investment mana...   \n", " 2   (age\u00a064)                                             Lawyer   \n", " 3   (age\u00a056)  U.S. Marine Corps officerLawyerAssistant Secre...   \n", " 4   (age\u00a045)  Social workerPolitical activistLawyerCollege p...   \n", " ..       ...                                                ...   \n", " 95  (age\u00a067)  College career counselorDirector, state Board ...   \n", " 96  (age\u00a066)                      AccountantCorporate executive   \n", " 97  (age\u00a059)                                             Lawyer   \n", " 98  (age\u00a068)  Orthopedic surgeonMedical chief of staffNonpro...   \n", " 99  (age\u00a066)                                             Lawyer   \n", " \n", "                            Previous electiveoffice(s)  \\\n", " 0                            U.S. HouseAlabama Senate   \n", " 1                                                None   \n", " 2                     Alaska House of Representatives   \n", " 3                             Alaska Attorney General   \n", " 4   U.S. HouseArizona SenateArizona House of Repre...   \n", " ..                                                ...   \n", " 95         U.S. HouseWest Virginia House of Delegates   \n", " 96                                               None   \n", " 97  U.S. HouseWisconsin AssemblyDane County, Wisco...   \n", " 98                                     Wyoming Senate   \n", " 99  U.S. HouseWyoming TreasurerWyoming SenateWyomi...   \n", " \n", "                                             Education        Assumed office  \\\n", " 0   University of Alabama Birmingham School of Law...       January 3, 1987   \n", " 1                        Southern Arkansas University       January 3, 2021   \n", " 2   Georgetown University Willamette University Co...  December 20, 2002[d]   \n", " 3   Culver Military Academy Harvard University Geo...       January 3, 2015   \n", " 4                            Brigham Young University       January 3, 2019   \n", " ..                                                ...                   ...   \n", " 95             Duke University University of Virginia       January 3, 2015   \n", " 96                            University of Minnesota       January 3, 2011   \n", " 97              Smith College University of Wisconsin       January 3, 2013   \n", " 98  Rensselaer Polytechnic University Georgetown U...      June 25, 2007[y]   \n", " 99                              University of Wyoming       January 3, 2021   \n", " \n", "     Term up      Residence  \n", " 0      2022  Tuscaloosa[3]  \n", " 1      2026         Auburn  \n", " 2      2022    Girdwood[4]  \n", " 3      2026   Anchorage[5]  \n", " 4      2024     Phoenix[6]  \n", " ..      ...            ...  \n", " 95     2026     Charleston  \n", " 96     2022        Oshkosh  \n", " 97     2024        Madison  \n", " 98     2024         Casper  \n", " 99     2026       Cheyenne  \n", " \n", " [100 rows x 12 columns]]"]}, "execution_count": 62, "metadata": {}, "output_type": "execute_result"}], "source": ["pandas_result = pd.read_html(table_string)\n", "pandas_result"]}, {"cell_type": "markdown", "id": "1395cbb9-b6c6-48a6-8cbc-80f39352bb2c", "metadata": {"index": 58}, "source": ["The above about looks sort of funny. \n", "\n", "The developers of pandas made the choice that `read_html` will always about a `list` of dataframes. This did this with the thinking that you could pass `read_html` the html of an entire webpage, and pandas will parse every `<table>` tag found in the html. \n", "\n", "In this case, we have already isolated the table we want which means the list given to us by `read_html` has a length of `1`."]}, {"cell_type": "code", "execution_count": 64, "id": "e67d21d9-f460-4fbe-98b1-6bf9475423ba", "metadata": {"index": 59}, "outputs": [{"data": {"text/plain": ["1"]}, "execution_count": 64, "metadata": {}, "output_type": "execute_result"}], "source": ["len(pandas_result)"]}, {"cell_type": "markdown", "id": "39d805cb-d0fe-4b6f-bd24-6df8ca7d943b", "metadata": {"index": 61}, "source": ["So really, all we need to do is **index** the list returned to us by `read_html`."]}, {"cell_type": "code", "execution_count": 65, "id": "03dc2bc1-9ba5-4843-88c8-04fb632f619e", "metadata": {"index": 62}, "outputs": [{"data": {"text/html": ["<div>\n", "<style scoped>\n", "    .dataframe tbody tr th:only-of-type {\n", "        vertical-align: middle;\n", "    }\n", "\n", "    .dataframe tbody tr th {\n", "        vertical-align: top;\n", "    }\n", "\n", "    .dataframe thead th {\n", "        text-align: right;\n", "    }\n", "</style>\n", "<table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", "    <tr style=\"text-align: right;\">\n", "      <th></th>\n", "      <th>State</th>\n", "      <th>Portrait</th>\n", "      <th>Senator</th>\n", "      <th>Party</th>\n", "      <th>Party.1</th>\n", "      <th>Born</th>\n", "      <th>Occupation(s)</th>\n", "      <th>Previous electiveoffice(s)</th>\n", "      <th>Education</th>\n", "      <th>Assumed office</th>\n", "      <th>Term up</th>\n", "      <th>Residence</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <th>0</th>\n", "      <td>Alabama</td>\n", "      <td>NaN</td>\n", "      <td>Richard Shelby</td>\n", "      <td>NaN</td>\n", "      <td>Republican[2]</td>\n", "      <td>(age\u00a087)</td>\n", "      <td>Lawyer</td>\n", "      <td>U.S. HouseAlabama Senate</td>\n", "      <td>University of Alabama Birmingham School of Law...</td>\n", "      <td>January 3, 1987</td>\n", "      <td>2022</td>\n", "      <td>Tuscaloosa[3]</td>\n", "    </tr>\n", "    <tr>\n", "      <th>1</th>\n", "      <td>Alabama</td>\n", "      <td>NaN</td>\n", "      <td>Tommy Tuberville</td>\n", "      <td>NaN</td>\n", "      <td>Republican</td>\n", "      <td>(age\u00a066)</td>\n", "      <td>College football coachPartner, investment mana...</td>\n", "      <td>None</td>\n", "      <td>Southern Arkansas University</td>\n", "      <td>January 3, 2021</td>\n", "      <td>2026</td>\n", "      <td>Auburn</td>\n", "    </tr>\n", "  </tbody>\n", "</table>\n", "</div>"], "text/plain": ["     State  Portrait           Senator  Party        Party.1      Born  \\\n", "0  Alabama       NaN    Richard Shelby    NaN  Republican[2]  (age\u00a087)   \n", "1  Alabama       NaN  Tommy Tuberville    NaN     Republican  (age\u00a066)   \n", "\n", "                                       Occupation(s)  \\\n", "0                                             Lawyer   \n", "1  College football coachPartner, investment mana...   \n", "\n", "  Previous electiveoffice(s)  \\\n", "0   U.S. HouseAlabama Senate   \n", "1                       None   \n", "\n", "                                           Education   Assumed office  \\\n", "0  University of Alabama Birmingham School of Law...  January 3, 1987   \n", "1                       Southern Arkansas University  January 3, 2021   \n", "\n", "   Term up      Residence  \n", "0     2022  Tuscaloosa[3]  \n", "1     2026         Auburn  "]}, "execution_count": 65, "metadata": {}, "output_type": "execute_result"}], "source": ["senate = pd.read_html(table_string)[0]\n", "senate.head(2)"]}, {"cell_type": "markdown", "id": "620e89c2-8be5-46f9-b201-68eca6e33eeb", "metadata": {"index": 64}, "source": ["And we have our data! \ud83e\udd73\n", "\n", "It is worth noting that the data given to us is a bit messy, and in some cases is missing information we might want. This pandas trick can be really powerful and helpful if we're short on time, but typically a manual scrape will always produce a cleaner result. "]}, {"cell_type": "markdown", "id": "b0d64bc5", "metadata": {"index": 65}, "source": ["## A better scrape:\n", "\n", "Below, we create a function called `scrape_senate_table` that produced a much cleaner dataframe.\n", "\n", "This function is completed for you. We encourage you to read through the comments, and even copy the code into a code cell and play around with it. "]}, {"cell_type": "code", "execution_count": 75, "id": "1bff3816", "metadata": {"index": 66}, "outputs": [], "source": ["# Run this cell unchanged\n", "def scrape_senate_table():\n", "    \n", "    # Connect to the wikipedia page\n", "    response = get('https://en.wikipedia.org/wiki/List_of_current_United_States_senators')\n", "    # Collect the html from the page\n", "    html = response.text\n", "    # Parse the html with BeautifulSoup\n", "    soup = BeautifulSoup(html)\n", "    # Find the senators table\n", "    table = soup.find('table', {'id': 'senators'})\n", "    \n", "    # The first row in the table contains the column names.\n", "    # Isolate the first row, then final all of the column tags.\n", "    columns = table.find('tr').find_all('th')\n", "    # Collect the row tags for the entire dataset\n", "    rows = table.find_all('tr')[1:]\n", "    # Create an empty list to append the row data to\n", "    senate_data = []\n", "    # Create some cleaning functions for text data\n", "    remove_new_line = lambda x: x.replace('\\n', '')\n", "    split_new_line = lambda x: x.split('\\n')\n", "    \n", "    # Loop over each row\n", "    for row in rows:\n", "        # find all td tags from the row\n", "        td_tags = row.find_all('td')\n", "        # ===============================================================================\n", "        # The senators table merges the cell for `state name` so it spans both \n", "        # senators from a state. \n", "        # When parsing the html, the state name only appears for the senator\n", "        # that appears first in the table, and the second appearing senator has \n", "        # one less td tag.\n", "        # Because of this we need to check the length of the td tags\n", "        # If there is one less tag, we add  the state name from the previous iteration\n", "        # To the beginning of the list of tags\n", "        # ===============================================================================\n", "        # Check if the list of tags has the full number of columns\n", "        if len(td_tags) == len(columns):\n", "            # If it does, store the first element in the list of tags\n", "            # to a variable called previous_element\n", "            previous_element = td_tags[0]\n", "        # If the list of tags does not have the full number of columns\n", "        # insert the previous_element variable at the beginning of the list of tags\n", "        else:\n", "            td_tags.insert(0, previous_element)\n", "        \n", "        # ===============================================================================\n", "        #                              Parse the row data\n", "        # ===============================================================================\n", "        # Collect the state name\n", "        state = remove_new_line(td_tags[0].text)\n", "        # Collect the image url\n", "        image = td_tags[1].find('img').attrs['src']\n", "        # Collect the name of the senator\n", "        name = remove_new_line(row.find('th').text)\n", "        # Collect the css color string for the political party\n", "        party_color = td_tags[2].attrs['style'].split(':')[-1]\n", "        # Collect the party name\n", "        party_name = remove_new_line(td_tags[3].text)\n", "        # Remove the [2] that occasionally follows the word \"Republicans\"\n", "        party_name = party_name.split('[')[0]\n", "        # Collect the date of birth\n", "        dob = ' '.join(remove_new_line(td_tags[4]\\\n", "                                       .text)\\\n", "                                       .strip()\\\n", "                                       .split(' ')[1:4])\n", "        # Collect the occupation\n", "        occupation = split_new_line(BeautifulSoup(str(td_tags[5])\\\n", "                                                  .replace('<br/>', '\\n'))\\\n", "                                                  .td\\\n", "                                                  .text)\n", "        # If only one occupation is present\n", "        # Pull that occupation out of the list\n", "        # And return a single string\n", "        if occupation[1] == '':\n", "            occupation = occupation[0]\n", "        # Collect the previous office\n", "        previous_office = split_new_line(BeautifulSoup(str(td_tags[6])\\\n", "                                                       .replace('<br/>', '\\n'))\\\n", "                                                       .td\\\n", "                                                      .text)\n", "        # If only one previous office is present\n", "        # Pull that value out of the list\n", "        # And return a single string\n", "        if previous_office[1] == '':\n", "            previous_office = previous_office[0]\n", "        # Collect assumed office\n", "        assumed_office = remove_new_line(td_tags[7].text)\n", "        # Collect the end of their term\n", "        term_up = remove_new_line(td_tags[8].text)\n", "        # Remove any [number] appendix references\n", "        term_up = term_up.split('[')[0]\n", "        # Collect their residence\n", "        residence = split_new_line(td_tags[9].text)\n", "        \n", "        # If only one residence is present\n", "        # Pull that value out of the list\n", "        # And return a single string\n", "        if residence[1] == '':\n", "            residence = residence[0]\n", "            # Many of the residences have a \n", "            # link pointing to additional information\n", "            # We do not need this\n", "            if '[' in residence:\n", "                residence = residence[:-3]\n", "        # Create data dictionary\n", "        collected = {'state': state, 'name': name,'dob': dob, 'party': party_name,\n", "                    'party_color': party_color,\n", "                    'occupation': occupation,\n", "                    'previous_office': previous_office,\n", "                    'assumed_office': assumed_office,\n", "                    'term_up': term_up, 'residence': residence,\n", "                    'portrait': image}\n", "        # Append dictionary to the senate_data list\n", "        senate_data.append(collected)\n", "        \n", "    # Once data from all rows has been collected\n", "    # return the data as a pandas dataframe\n", "    return pd.DataFrame(senate_data)"]}, {"cell_type": "code", "execution_count": 76, "id": "3a53ffc9", "metadata": {"index": 68}, "outputs": [{"data": {"text/html": ["<div>\n", "<style scoped>\n", "    .dataframe tbody tr th:only-of-type {\n", "        vertical-align: middle;\n", "    }\n", "\n", "    .dataframe tbody tr th {\n", "        vertical-align: top;\n", "    }\n", "\n", "    .dataframe thead th {\n", "        text-align: right;\n", "    }\n", "</style>\n", "<table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", "    <tr style=\"text-align: right;\">\n", "      <th></th>\n", "      <th>state</th>\n", "      <th>name</th>\n", "      <th>dob</th>\n", "      <th>party</th>\n", "      <th>party_color</th>\n", "      <th>occupation</th>\n", "      <th>previous_office</th>\n", "      <th>assumed_office</th>\n", "      <th>term_up</th>\n", "      <th>residence</th>\n", "      <th>portrait</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <th>0</th>\n", "      <td>Alabama</td>\n", "      <td>Richard Shelby</td>\n", "      <td>May 6, 1934</td>\n", "      <td>Republican</td>\n", "      <td>#E81B23</td>\n", "      <td>Lawyer</td>\n", "      <td>[U.S. House, Alabama Senate, ]</td>\n", "      <td>University of AlabamaBirmingham School of LawU...</td>\n", "      <td>January 3, 1987</td>\n", "      <td>2022</td>\n", "      <td>//upload.wikimedia.org/wikipedia/commons/thumb...</td>\n", "    </tr>\n", "    <tr>\n", "      <th>1</th>\n", "      <td>Alabama</td>\n", "      <td>Tommy Tuberville</td>\n", "      <td>September 18, 1954</td>\n", "      <td>Republican</td>\n", "      <td>#E81B23</td>\n", "      <td>[College football coach, Partner, investment m...</td>\n", "      <td>None</td>\n", "      <td>Southern Arkansas University</td>\n", "      <td>January 3, 2021</td>\n", "      <td>2026</td>\n", "      <td>//upload.wikimedia.org/wikipedia/commons/thumb...</td>\n", "    </tr>\n", "    <tr>\n", "      <th>2</th>\n", "      <td>Alaska</td>\n", "      <td>Lisa Murkowski</td>\n", "      <td>May 22, 1957</td>\n", "      <td>Republican</td>\n", "      <td>#E81B23</td>\n", "      <td>Lawyer</td>\n", "      <td>Alaska House of Representatives</td>\n", "      <td>Georgetown UniversityWillamette University Col...</td>\n", "      <td>December 20, 2002</td>\n", "      <td>2022</td>\n", "      <td>//upload.wikimedia.org/wikipedia/commons/thumb...</td>\n", "    </tr>\n", "    <tr>\n", "      <th>3</th>\n", "      <td>Alaska</td>\n", "      <td>Dan Sullivan</td>\n", "      <td>November 13, 1964</td>\n", "      <td>Republican</td>\n", "      <td>#E81B23</td>\n", "      <td>[U.S. Marine Corps officer, Lawyer, Assistant ...</td>\n", "      <td>Alaska Attorney General</td>\n", "      <td>Culver Military AcademyHarvard UniversityGeorg...</td>\n", "      <td>January 3, 2015</td>\n", "      <td>2026</td>\n", "      <td>//upload.wikimedia.org/wikipedia/commons/thumb...</td>\n", "    </tr>\n", "    <tr>\n", "      <th>4</th>\n", "      <td>Arizona</td>\n", "      <td>Kyrsten Sinema</td>\n", "      <td>July 12, 1976</td>\n", "      <td>Democratic</td>\n", "      <td>#3333FF</td>\n", "      <td>[Social worker, Political activist, Lawyer, Co...</td>\n", "      <td>[U.S. House, Arizona Senate, Arizona House of ...</td>\n", "      <td>Brigham Young University</td>\n", "      <td>January 3, 2019</td>\n", "      <td>2024</td>\n", "      <td>//upload.wikimedia.org/wikipedia/commons/thumb...</td>\n", "    </tr>\n", "    <tr>\n", "      <th>...</th>\n", "      <td>...</td>\n", "      <td>...</td>\n", "      <td>...</td>\n", "      <td>...</td>\n", "      <td>...</td>\n", "      <td>...</td>\n", "      <td>...</td>\n", "      <td>...</td>\n", "      <td>...</td>\n", "      <td>...</td>\n", "      <td>...</td>\n", "    </tr>\n", "    <tr>\n", "      <th>95</th>\n", "      <td>West Virginia</td>\n", "      <td>Shelley Moore Capito</td>\n", "      <td>November 26, 1953</td>\n", "      <td>Republican</td>\n", "      <td>#E81B23</td>\n", "      <td>[College career counselor, Director, state Boa...</td>\n", "      <td>[U.S. House, West Virginia House of Delegates, ]</td>\n", "      <td>Duke UniversityUniversity of Virginia</td>\n", "      <td>January 3, 2015</td>\n", "      <td>2026</td>\n", "      <td>//upload.wikimedia.org/wikipedia/commons/thumb...</td>\n", "    </tr>\n", "    <tr>\n", "      <th>96</th>\n", "      <td>Wisconsin</td>\n", "      <td>Ron Johnson</td>\n", "      <td>April 8, 1955</td>\n", "      <td>Republican</td>\n", "      <td>#E81B23</td>\n", "      <td>[Accountant, Corporate executive, ]</td>\n", "      <td>None</td>\n", "      <td>University of Minnesota</td>\n", "      <td>January 3, 2011</td>\n", "      <td>2022</td>\n", "      <td>//upload.wikimedia.org/wikipedia/commons/thumb...</td>\n", "    </tr>\n", "    <tr>\n", "      <th>97</th>\n", "      <td>Wisconsin</td>\n", "      <td>Tammy Baldwin</td>\n", "      <td>February 11, 1962</td>\n", "      <td>Democratic</td>\n", "      <td>#3333FF</td>\n", "      <td>Lawyer</td>\n", "      <td>[U.S. House, Wisconsin Assembly, Dane County, ...</td>\n", "      <td>Smith CollegeUniversity of Wisconsin</td>\n", "      <td>January 3, 2013</td>\n", "      <td>2024</td>\n", "      <td>//upload.wikimedia.org/wikipedia/commons/thumb...</td>\n", "    </tr>\n", "    <tr>\n", "      <th>98</th>\n", "      <td>Wyoming</td>\n", "      <td>John Barrasso</td>\n", "      <td>July 21, 1952</td>\n", "      <td>Republican</td>\n", "      <td>#E81B23</td>\n", "      <td>[Orthopedic surgeon, Medical chief of staff, N...</td>\n", "      <td>Wyoming Senate</td>\n", "      <td>Rensselaer Polytechnic UniversityGeorgetown Un...</td>\n", "      <td>June 25, 2007</td>\n", "      <td>2024</td>\n", "      <td>//upload.wikimedia.org/wikipedia/commons/thumb...</td>\n", "    </tr>\n", "    <tr>\n", "      <th>99</th>\n", "      <td>Wyoming</td>\n", "      <td>Cynthia Lummis</td>\n", "      <td>September 10, 1954</td>\n", "      <td>Republican</td>\n", "      <td>#E81B23</td>\n", "      <td>Lawyer</td>\n", "      <td>[U.S. House, Wyoming Treasurer, Wyoming Senate...</td>\n", "      <td>University of Wyoming</td>\n", "      <td>January 3, 2021</td>\n", "      <td>2026</td>\n", "      <td>//upload.wikimedia.org/wikipedia/commons/thumb...</td>\n", "    </tr>\n", "  </tbody>\n", "</table>\n", "<p>100 rows \u00d7 11 columns</p>\n", "</div>"], "text/plain": ["            state                  name                 dob       party  \\\n", "0         Alabama        Richard Shelby         May 6, 1934  Republican   \n", "1         Alabama      Tommy Tuberville  September 18, 1954  Republican   \n", "2          Alaska        Lisa Murkowski        May 22, 1957  Republican   \n", "3          Alaska          Dan Sullivan   November 13, 1964  Republican   \n", "4         Arizona        Kyrsten Sinema       July 12, 1976  Democratic   \n", "..            ...                   ...                 ...         ...   \n", "95  West Virginia  Shelley Moore Capito   November 26, 1953  Republican   \n", "96      Wisconsin           Ron Johnson       April 8, 1955  Republican   \n", "97      Wisconsin         Tammy Baldwin   February 11, 1962  Democratic   \n", "98        Wyoming         John Barrasso       July 21, 1952  Republican   \n", "99        Wyoming        Cynthia Lummis  September 10, 1954  Republican   \n", "\n", "   party_color                                         occupation  \\\n", "0      #E81B23                                             Lawyer   \n", "1      #E81B23  [College football coach, Partner, investment m...   \n", "2      #E81B23                                             Lawyer   \n", "3      #E81B23  [U.S. Marine Corps officer, Lawyer, Assistant ...   \n", "4      #3333FF  [Social worker, Political activist, Lawyer, Co...   \n", "..         ...                                                ...   \n", "95     #E81B23  [College career counselor, Director, state Boa...   \n", "96     #E81B23                [Accountant, Corporate executive, ]   \n", "97     #3333FF                                             Lawyer   \n", "98     #E81B23  [Orthopedic surgeon, Medical chief of staff, N...   \n", "99     #E81B23                                             Lawyer   \n", "\n", "                                      previous_office  \\\n", "0                      [U.S. House, Alabama Senate, ]   \n", "1                                                None   \n", "2                     Alaska House of Representatives   \n", "3                             Alaska Attorney General   \n", "4   [U.S. House, Arizona Senate, Arizona House of ...   \n", "..                                                ...   \n", "95   [U.S. House, West Virginia House of Delegates, ]   \n", "96                                               None   \n", "97  [U.S. House, Wisconsin Assembly, Dane County, ...   \n", "98                                     Wyoming Senate   \n", "99  [U.S. House, Wyoming Treasurer, Wyoming Senate...   \n", "\n", "                                       assumed_office            term_up  \\\n", "0   University of AlabamaBirmingham School of LawU...    January 3, 1987   \n", "1                        Southern Arkansas University    January 3, 2021   \n", "2   Georgetown UniversityWillamette University Col...  December 20, 2002   \n", "3   Culver Military AcademyHarvard UniversityGeorg...    January 3, 2015   \n", "4                            Brigham Young University    January 3, 2019   \n", "..                                                ...                ...   \n", "95              Duke UniversityUniversity of Virginia    January 3, 2015   \n", "96                            University of Minnesota    January 3, 2011   \n", "97               Smith CollegeUniversity of Wisconsin    January 3, 2013   \n", "98  Rensselaer Polytechnic UniversityGeorgetown Un...      June 25, 2007   \n", "99                              University of Wyoming    January 3, 2021   \n", "\n", "   residence                                           portrait  \n", "0       2022  //upload.wikimedia.org/wikipedia/commons/thumb...  \n", "1       2026  //upload.wikimedia.org/wikipedia/commons/thumb...  \n", "2       2022  //upload.wikimedia.org/wikipedia/commons/thumb...  \n", "3       2026  //upload.wikimedia.org/wikipedia/commons/thumb...  \n", "4       2024  //upload.wikimedia.org/wikipedia/commons/thumb...  \n", "..       ...                                                ...  \n", "95      2026  //upload.wikimedia.org/wikipedia/commons/thumb...  \n", "96      2022  //upload.wikimedia.org/wikipedia/commons/thumb...  \n", "97      2024  //upload.wikimedia.org/wikipedia/commons/thumb...  \n", "98      2024  //upload.wikimedia.org/wikipedia/commons/thumb...  \n", "99      2026  //upload.wikimedia.org/wikipedia/commons/thumb...  \n", "\n", "[100 rows x 11 columns]"]}, "execution_count": 76, "metadata": {}, "output_type": "execute_result"}], "source": ["# Run this cell unchanged\n", "scrape_senate_table()"]}, {"cell_type": "code", "execution_count": null, "id": "761819f1", "metadata": {"index": 70}, "outputs": [], "source": []}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.4"}}, "nbformat": 4, "nbformat_minor": 5}